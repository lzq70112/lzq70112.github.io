---
title: kubeasz3部署K8S-node篇
date: 2021-09-01 10:13:16
permalink: /pages/b5e1547/
categories:
  - 《K8S》学习笔记
tags:
  - k8s
---
kubeasz3部署K8S-node篇  
<!-- more -->



## 1、原理
- Kubelet 

负责本Node节点上的Pod的创建、修改、监控、删除等全生命周期管理，同时Kubelet定时“上报”本Node的状态信息到API Server里。

- Proxy 

实现了Service的代理与软件模式的负载均衡器。

客户端通过Kubectl命令行工具或Kubectl Proxy来访问Kubernetes系统，在Kubernetes集群内部的客户端可以直接使用Kuberctl命令管理集群。Kubectl Proxy是API Server的一个反向代理，在Kubernetes集群外部的客户端可以通过Kubernetes Proxy来访问API Server。

API Server内部有一套完备的安全机制，包括认证、授权和准入控制等相关模块。







## 2、kubeasz3部署K8S-proxy配置

kubeasz3部署K8S-proxy配置、会在每个非master的节点上部署haproxy 指向master



` vim /etc/haproxy/haproxy.cfg   `

```shell
listen kube_master
        bind 127.0.0.1:6443
        mode tcp
        option tcplog
        option dontlognull
        option dontlog-normal
        balance roundrobin
        server 192.168.2.22 192.168.2.22:6443 check inter 10s fall 2 rise 2 weight 1
        server 192.168.2.23 192.168.2.23:6443 check inter 10s fall 2 rise 2 weight 1
```



**kube-proxy ipvs和iptables的异同**

iptables与IPVS都是基于Netfilter实现的，但因为定位不同，二者有着本质的差别：iptables是为防火墙而设计的；IPVS则专门用于高性能负载均衡，并使用更高效的数据结构（Hash表），允许几乎无限的规模扩张。

与iptables相比，IPVS拥有以下明显优势：

- 为大型集群提供了更好的可扩展性和性能；
- 支持比iptables更复杂的复制均衡算法（最小负载、最少连接、加权等）；
- 支持服务器健康检查和连接重试等功能；
- 可以动态修改ipset的集合，即使iptables的规则正在使用这个集合。

### 2-1、iptables

iptables作为kube-proxy的默认模式。iptables模式下的kube-proxy不再起到Proxy的作用，其核心功能：通过API Server的Watch接口实时跟踪Service与Endpoint的变更信息，并更新对应的iptables规则，Client的请求流量则通过iptables的NAT机制“直接路由”到目标Pod。



![img](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/121317001370.png)



### 2-2、ipvs说明及



**LVS调度算法：**

**1. 轮叫调度 rr**
这种算法是最简单的，就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是简单。轮询算法假设所有的服务器处理请求的能力都是一样的，调度器会将所有的请求平均分配给每个真实服务器，不管后端 RS 配置和处理能力，非常均衡地分发下去。

**2. 加权轮叫 wrr**
这种算法比 rr 的算法多了一个权重的概念，可以给 RS 设置权重，权重越高，那么分发的请求数越多，权重的取值范围 0 – 100。主要是对rr算法的一种优化和补充， LVS 会考虑每台服务器的性能，并给每台服务器添加要给权值，如果服务器A的权值为1，服务器B的权值为2，则调度到服务器B的请求会是服务器A的2倍。权值越高的服务器，处理的请求越多。

**3. 最少链接 lc**
这个算法会根据后端 RS 的连接数来决定把请求分发给谁，比如 RS1 连接数比 RS2 连接数少，那么请求就优先发给 RS1

**4. 加权最少链接 wlc**
这个算法比 lc 多了一个权重的概念。

**5. 基于局部性的最少连接调度算法 lblc**
这个算法是请求数据包的目标 IP 地址的一种调度算法，该算法先根据请求的目标 IP 地址寻找最近的该目标 IP 地址所有使用的服务器，如果这台服务器依然可用，并且有能力处理该请求，调度器会尽量选择相同的服务器，否则会继续选择其它可行的服务器

**6. 复杂的基于局部性最少的连接算法 lblcr**
记录的不是要给目标 IP 与一台服务器之间的连接记录，它会维护一个目标 IP 到一组服务器之间的映射关系，防止单点服务器负载过高。

**7. 目标地址散列调度算法 dh**
该算法是根据目标 IP 地址通过散列函数将目标 IP 与服务器建立映射关系，出现服务器不可用或负载过高的情况下，发往该目标 IP 的请求会固定发给该服务器。

**8. 源地址散列调度算法 sh**
与目标地址散列调度算法类似，但它是根据源地址散列算法进行静态分配固定的服务器资源。



**流程**



![img](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/clipboard.png)

**k8s的service和endpoine是如何关联和相互影响的？**
1、 api-server创建service对象，与service绑定的pod地址：称之为endpoints（kubectl get ep可以查看）
2、服务发现方面：kube-proxy监控service后端endpoint的动态变化，并且维护service和endpoint的映射关系



### 2-3、修改轮巡方式

`vim /etc/systemd/system/kube-proxy.service ` # 修改轮巡方式一

```
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
# kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后，kube-proxy 会对访问 Service IP 的请求做 SNAT
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/opt/kube/bin/kube-proxy \
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml \
  #--ipvs-scheduler=sh
  # 修改ipvs 调度轮巡方式
Restart=always
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
~                               
```



 `vim /var/lib/kube-proxy/kube-proxy-config.yaml` #如果是/etc/systemd/system/kube-proxy.service 中是以yaml加载配置可以改

```
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 192.168.2.25
clientConnection:
  kubeconfig: "/etc/kubernetes/kube-proxy.kubeconfig"
clusterCIDR: "172.20.0.0/16"
conntrack:
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
healthzBindAddress: 192.168.2.25:10256
hostnameOverride: "192.168.2.25"
metricsBindAddress: 192.168.2.25:10249
mode: "ipvs"
kubeProxyIPTablesConfiguration:
  masqueradeAll: false
kubeProxyIPVSConfiguration:
  scheduler: sh
  excludeCIDRs: []
```







`ipvsadm -L -n` #查看算法调度

