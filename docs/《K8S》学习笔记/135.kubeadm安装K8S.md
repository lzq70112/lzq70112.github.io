---
title: kubeadm安装K8S集群实践
date: 2022-06-03 10:13:16
permalink: /pages/k8s135/
categories:
  - 《K8S》学习笔记
tags:
  - k8s
---




文档 https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

## 1、安装docker

## 2、安装yum源

```sh
sudo kubeadm reset
sudo rm /etc/cni/net.d -fr
# 格式化残留
```



```sh
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum makecache #生成缓存
```

```
yum install -y --nogpgcheck kubelet-1.20.2  kubeadm-1.20.2  kubectl-1.20.2
```

关闭缓存

```sh
swapoff -a
```

```sh
vim /etc/fstab 
```

```sh
#/dev/mapper/centos-swap swap                    swap    defaults        0 0
# 注释掉此行
```

修改内核

```sh
echo 1 > /proc/sys/net/ipv4/ip_forward
#modprobe br_netfilter
echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables

#然后把kubelet 设置为开机启动
systemctl daemon-reload
systemctl enable kubelet
```

## 3、初始化

```sh
 kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers  --kubernetes-version=1.20.2 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12  
```

```sh
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

#加入集群
kubeadm join 192.168.2.22:6443 --token lt5kzq.6cyrq5hfo4piaakf \
    --discovery-token-ca-cert-hash sha256:024a7fe4b9ee4d9f4fd177cd44fd26a2b508da65c1f23bcffb3d89646da47628 

```

完成后

```sh
kubectl taint nodes --all node-role.kubernetes.io/master-   #(后面一个 – 是需要的)

#给 工作节点打标签
 kubectl label node node2 node-role.kubernetes.io/node=node
```

## 4、安装flannel的网络插件

文档 https://github.com/coreos/flannel 

```sh
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml #部署
```

查看`coredns`是否启动

```sh
vim /etc/kubernetes/manifests/kube-controller-manager.yaml #查看初始化地址是否正确
```

```
    - --cluster-cidr=10.244.0.0/16
    - --allocate-node-cidrs=true
```



## 5、导入rancher

```sh
docker run -itd --privileged  -p 8443:443 \
-v /data/rancher:/var/lib/rancher \
--restart=unless-stopped  -e CATTLE_AGENT_IMAGE="registry.cn-hangzhou.aliyuncs.com/rancher/rancher-agent:v2.6.6"  registry.cn-hangzhou.aliyuncs.com/rancher/rancher:v2.6.6
```



导入已有集群

![image-20220718090055400](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207180900751.png)

```sh
curl --insecure -sfL https://192.168.2.23:8443/v3/import/xbd2sn5zvhc82nxkmc4nsz5kj9zn72scdtv8hs2qs6gsdjl775l74x_c-m-t7wqmx6q.yaml | kubectl apply -f -
```

```sh
kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user cluster-admin #创建用户
```









## 5、集群问题处理

```
vi /etc/kubernetes/manifests/kube-scheduler.yaml
vi /etc/kubernetes/manifests/kube-controller-manager.yaml
```

```sh
- --port=0 #删除
```

```
systemctl restart kubelet
kubectl get cs
```

```
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok                  
scheduler            Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
```

## 6、安装helm、ingress

文档 https://github.com/helm/helm/releases/tag/v3.4.0



```sh
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx #添加仓库
helm install my-nginx ingress-nginx/ingress-nginx  #安装ingress
```

## 7、授权和认证机制-证书篇-UA

### 7.1、创建用户账号生成证书

```sh
yum install openssl  openssl-devel #安装依赖
```

```sh
mkdir -p ./ua/xiongmao 
openssl genrsa -out client.key 2048  #  (这一步是生成客户端私钥)
openssl req -new -key client.key -out client.csr -subj "/CN=xiongmao"  #   (根据私钥生成csr， /CN指定了用户名xiongmao)
sudo openssl x509 -req -in client.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out client.crt -days 365 #(根据k8s的CA证书生成我们用户的客户端证书)
```

### 7.2、使用证书

常用

```sh
openssl x509 -noout -subject -in client.crt #解码证书
```

测试证书

```sh
curl --cert ./client.crt --key ./client.key --cacert /etc/kubernetes/pki/ca.crt -s https://192.168.2.22:6443/api #访问apiserver

#或者

curl --cert ./client.crt --key ./client.key --insecure -s https://192.168.2.22:6443/api
```

加入证书到`config`

```sh
kubectl config --kubeconfig=/root/.kube/config set-credentials xiongmao  --client-certificate=/root/ua/xiongmao/client.crt --client-key=/root/ua/xiongmao/client.key #加入设置证书到~/.kube/config
```

切换用户

```sh
kubectl config --kubeconfig=/root/.kube/config  set-context user_context --cluster=kubernetes  --user=xiongmao #添加用户

# 指定当前上下文是
 kubectl config --kubeconfig=/root/.kube/config use-context user_context #使用用户上下文

```

### 7.3、入门Role和RoleBinding、创建一个角色

```sh
 kubectl config  use-context kubernetes-admin@kubernetes #切换admin
```

创建role

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: mypod
rules:
- apiGroups: ["*"]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```

权限内容

| create | delete | deletecollection | get  | list | patch    | update | watch |
| ------ | ------ | ---------------- | ---- | ---- | -------- | :----: | :---: |
| 创建   | 删除   | 批量删除         | 获取 | 列表 | 合并变更 |  更新  | 监听  |



​    查看资源类型

```sh
    kubectl api-resources -o wide #当前集群的所有资源
```



### 7.4、 rolebinding绑定角色和用户

```sh
kubectl create rolebinding mypodbinding  -n default  --role mypod --user xiongmao
```

或者

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: null
  name: mypodrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mypod
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: xiongmao
```

### 7.5、role、RoleBinding、clster-role、ClusterRoleBinding的区别

- Role (角色)

  它可以包含一堆权限。用于授予对单个命名空间的资源访问

- RoleBinding 

  顾名思义，将用户和角色进行绑定

- cluster-role

   管理集群中多个 namespace,就需要使用到clusterrole

- ClusterRoleBinding

​      对多个空间进行控制

```
kubectl get cluster-role
```

#### 7.5.1、当ClusterRole 关联RoleBinding 只能访问RoleBinding的指定空间

创建cluser-role

```yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: mypod-cluster
rules:
- apiGroups: ["*"]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```

创建RoleBinding

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: mypodrolebinding-cluster
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: mypod-cluster
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: xiongmao
```

当ClusterRole 关联RoleBinding 只能访问RoleBinding的指定空间



#### 7.5.2、当ClusterRole 关联ClusterRoleBinding 可以访问所有空间

```YAML
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: mypod-clusterrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: mypod-cluster
subjects:
 - apiGroup: rbac.authorization.k8s.io
   kind: User
   name: xiongmao
```

### 7.6、常用命令汇总

```sh
kubectl get rolebinding 
kubectl get role 
kubectl get clusterrolebinding 
kubectl get clusterrole 
kubectl api-resources -o wide #当前集群的所有资源
#切换角色 

 kubectl config --kubeconfig=/root/.kube/config use-context user_context #使用用户上下文
 kubectl config  use-context kubernetes-admin@kubernetes #切换admin
```

## 8、授权和认证机制-token篇-SA

### 8.1、生成token

```sh
head -c 16 /dev/urandom | od -An -t x | tr -d ' '        # 生成token
kubectl config set-credentials xiongmao --token=4e2f6f4250a43ce94426b6264dad2609
```

### 8.2、授权

```
vi /etc/kubernetes/pki/token_auth 
```

```
4e2f6f4250a43ce94426b6264dad2609,xiongmao,1001
```

修改启动参数

```sh
sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml
```

```sh
#加入  
--token-auth-file=/etc/kubernetes/pki/token_auth
```

### 8.3、验证

```sh
curl -H "Authorization: Bearer 4e2f6f4250a43ce94426b6264dad2609" https://192.168.2.2:6443/api/v1/namespaces/default/pods --insecure
```

### 8.4、创建sa并绑定权限

```sh
kubectl  create  serviceaccount  mysa   #创建sa
```

```sh
kubectl create clusterrolebinding mysa-crb --clusterrole=mypod-cluster --serviceaccount=default:mysa #绑定角色和sa
```

```sh
kubectl get secret  $(kubectl get sa  mysa -o json | jq -Mr '.secrets[0].name') -o json | jq -Mr '.data.token' | base64 -d


#设置成一个变量
mysatoken=$(kubectl get secret  $(kubectl get sa  mysa -o json | jq -Mr '.secrets[0].name'
) -o json | jq -Mr '.data.token' | base64 -d)
 

#请求
curl -H "Authorization: Bearer $mysatoken" --insecure  https://192.168.2.22:6443/api/v1/namespaces/default/pods 

```

### 8.5、pod对sa权限的应用

创建一个pod

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myngx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      serviceAccountName: mysa
      # sa
      containers:
        - name: nginxtest
          image: nginx:1.18-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
```

进入容器测试

```sh
kubectl exec -it  myngx-7dfc9699cf-hvx7b -- sh #进入容器 一下内容都在容器执行
```

```sh
echo $KUBERNETES_SERVICE_HOST    # 内置变量指向api-server
echo $KUBERNETES_PORT_443_TCP_PORT # 内置变量
```

```sh
TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token` #内置变量sa
APISERVER="https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT"  #设置api

curl --header "Authorization: Bearer $TOKEN" --insecure -s $APISERVER/api/v1/namespaces/default/pods #测试请求
# 或者
curl --header "Authorization: Bearer $TOKEN" --cacert  /var/run/secrets/kubernetes.io/serviceaccount/ca.crt   $APISERVER/api/v1/namespaces/default/pods #指定ca.cert

```

## 9、pod基础应用

### 9.1、pod之间的存储共享

文档 [https://](https://kubernetes.io/zh/docs/concepts/storage/volumes/)[kubernetes.io/zh/docs/concepts/storage/volumes/](https://kubernetes.io/zh/docs/concepts/storage/volumes/)

  同一个pod内的容器都能读写EmptyDir中 文件。常用于临时空间、多容器共享，如日志或者tmp文件需要的临时目录

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myngx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: ngx
          image: nginx:1.18-alpine
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: sharedata
              mountPath: /data
        - name: alpine
          image: alpine:3.12
          imagePullPolicy: IfNotPresent
          command: ["sh","-c","echo this is alpine && sleep 36000"]
          volumeMounts:
            - name: sharedata
              mountPath: /data
      volumes:
        - name:  sharedata
          emptyDir: {}

```

### 9.2、pod的初始化容器

-  Init 容器是一种特殊容器，在 Pod 内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本

-  Init 容器与普通的容器非常像，除了如下两点：

​        它们总是运行到完成。

​        每个都必须在下一个启动之前成功完成。

   如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 restartPolicy 值为 "Never"，Kubernetes 不会重新启动 Pod。

文档地址: https://kubernetes.io/zh/docs/concepts/workloads/pods/init-containers/



```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myngx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      initContainers:
        - name: init-mydb
          image: alpine:3.12
          command: ['sh', '-c', 'echo wait for db && sleep 35 && echo done']
      containers:
        - name: ngx
          image: nginx:1.18-alpine
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: sharedata
              mountPath: /data
        - name: alpine
          image: alpine:3.12
          imagePullPolicy: IfNotPresent
          command: ["sh","-c","echo this is alpine && sleep 36000"]
          volumeMounts:
            - name: sharedata
              mountPath: /data
      volumes:
        - name:  sharedata
          emptyDir: {}

```

## 10、pod对configmap的引用

configmap的4个场景

  1、 容器 entrypoint 的命令行参数

  2、 容器的环境变量

  3、 映射成文件

  4、 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap

### 10.1、创建cm

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mycm
data:
  # 每一个键对应一个简单的值,以字符串的形式体现
  username: "xiongmao"
  userage: "19"
  user.info: |
    name=xiongmao
    age=19

```

### 10.2、引用环境变量

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myngx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: ngx
          image: nginx:1.18-alpine
          imagePullPolicy: IfNotPresent
          env:
            - name: TEST
              value: testvalue
            - name: USERNAME
              valueFrom:
                configMapKeyRef:
                  name: mycm    #  ConfigMap的名称
                  key: username # 需要取值的键
```

### 10.3、引用成单个文件

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myngx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: ngx
          image: nginx:1.18-alpine
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: cmdata
              mountPath: /data
          env:
            - name: TEST
              value: testvalue
            - name: USERNAME
              valueFrom:
                configMapKeyRef:
                  name: mycm    #  ConfigMap的名称
                  key: username # 需要取值的键
      volumes:
        - name: cmdata
          configMap:
            name: mycm
            items:
              - key: user.info
                path: user.txt
```

### 10.4、subpath的引用

功能与上述10.3一样，不指定`subPath`时引用全部文件，这时你会发现，所有文件都被 映射进去了，文件名就是key名

```
  subPath: user.info
```



```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myngx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: ngx
          image: nginx:1.18-alpine
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: cmdata
              mountPath: /data/user.txt
              subPath: user.info
              # 引用
          env:
            - name: TEST
              value: testvalue
            - name: USERNAME
              valueFrom:
                configMapKeyRef:
                  name: mycm
                  key: username 
      volumes:
        - name: cmdata
          configMap:
            defaultMode: 0655
            name: mycm
#            items:
#              - key: user.info
#                path: user.txt
```

### 10.5、实战--pod对sa应用获取对应的cm

创建sa

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cmuser
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cmrole
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cmclusterrolebinding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cmrole
subjects:
  - kind: ServiceAccount
    name: cmuser
    namespace: default
```

调用测试

```go
package main

import (
	"context"
	"fmt"
	"io/ioutil"
	"k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"log"
	"os"
)
var api_server string
var token string
func init() {
	api_server=fmt.Sprintf("https://%s:%s",
		os.Getenv("KUBERNETES_SERVICE_HOST"),os.Getenv("KUBERNETES_PORT_443_TCP_PORT"))
     f,err:=os.Open("/var/run/secrets/kubernetes.io/serviceaccount/token")
     if err!=nil{
     	log.Fatal(err)
	 }
     b,_:=ioutil.ReadAll(f)
     token=string(b)
}
func getClient() *kubernetes.Clientset{
	config:=&rest.Config{
		Host:api_server,
		BearerToken:token,
		TLSClientConfig:rest.TLSClientConfig{CAFile:"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"},
 	}
	c,err:=kubernetes.NewForConfig(config)
	if err!=nil{
		log.Fatal(err)
	}

	return c
}
func main() {
       cm,err:=getClient().CoreV1().ConfigMaps("default").
       	Get(context.Background(),"mycm",v1.GetOptions{})
       if err!=nil{
       	log.Fatal(err)
	   }
       for k,v:=range cm.Data{
       	   fmt.Printf("key=%s,value=%s\n",k,v)
	   }
	  select {}
      // 阻塞
}
```

交叉编译

```
set GOOS=linux
set GOARCH=amd64
go build -o cmtest cm.go
```

创建并上传编译文件生成对应的pod测试

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cdmtest
spec:
  selector:
    matchLabels:
      app: cmtest
  replicas: 1
  template:
    metadata:
      labels:
        app: cmtest
    spec:
      serviceAccount: cmuser
      # 指定sa
      nodeName: node1
      containers:
        - name: cmtest
          image: alpine:3.12
          imagePullPolicy: IfNotPresent
          command: ["/app/cmtest"]
          volumeMounts:
            - name: app
              mountPath: /app
      volumes:
        - name: app
          hostPath:
            path: /root/app
            type: Directory
```

![image-20220725001831137](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207250018302.png)

### 10.6、通过api-server对cm的监测

```sh
kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8009 #代理api
```



```go
package main

import (
	"k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/util/wait"
	"k8s.io/client-go/informers"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"log"
)

type CmHandler struct{}

func (this *CmHandler) OnAdd(obj interface{}) {}
func (this *CmHandler) OnUpdate(oldObj, newObj interface{}) {
	if newObj.(*v1.ConfigMap).Name == "mycm" {
		log.Println("mycm发生了变化")
	}
}
func (this *CmHandler) OnDelete(obj interface{}) {}

func getClient() *kubernetes.Clientset {
	config := &rest.Config{
		Host: "http://192.168.2.22:8009",
	}
	c, err := kubernetes.NewForConfig(config)
	if err != nil {
		log.Fatal(err)
	}

	return c
}
func main() {

	fact := informers.NewSharedInformerFactory(getClient(), 0)

	cmInformer := fact.Core().V1().ConfigMaps()
	cmInformer.Informer().AddEventHandler(&CmHandler{})

	fact.Start(wait.NeverStop)
	select {}
}
```

![image-20220725004902450](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207250049541.png)
