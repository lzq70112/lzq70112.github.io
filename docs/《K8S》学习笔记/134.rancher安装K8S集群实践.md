---
title: rancher安装K8S集群实践
date: 2022-06-03 10:13:16
permalink: /pages/k8s134/
categories:
  - 《K8S》学习笔记
tags:
  - k8s
  - cicd
  - elk
  - ingress
  - rancher
---

官方网站 https://www.rancher.cn/quick-start/

## 1、安装rancher

```
sudo docker run  --name rancher --privileged -d --restart=unless-stopped -p 880:80 -p 8443:443 rancher/rancher:stable
```

## 2、登录rancher

![login](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207011355068.gif)

## 3、创建一个单点集群

![login](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207011420267.gif)

## 4、创建一个服务

![image-20220701151442209](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207011514040.png)

## 5、创建一个go的项目

```go
package main

import (
	"github.com/gin-gonic/gin"
)

func main() {
	//Default返回一个默认的路由引擎
	r := gin.Default()
	r.Handle("GET", "/", func(c *gin.Context) {
		c.String(200, "hello,xiong mao")
	})
	r.Run(":80")
}

```

![1](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207031432612.gif)

![image-20220703143448628](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207031434776.png)

## 6、利用ingress-nginx对app负载

再另外一台节点镜像部署，其中app内容不太一致参考步骤五

![image-20220703171928549](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207031719827.png)

测试

![image-20220703173958690](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207031739944.png)

## 7、cluserip访问与测试

cluserip访问不在暴露端口

![image-20220703175815099](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207031758817.png)

```yaml
        name: container-0
        ports:
        - containerPort: 80
          name: myapp
          protocol: TCP
```

```go
package main

import (
	"github.com/gin-gonic/gin"
	"io/ioutil"
	"log"
	"net/http"
)

func main() {

	r := gin.Default()
	r.Handle("GET", "/", func(context *gin.Context) {
		host := context.Query("host")
		if host == "" {
			context.JSON(400, gin.H{"error": "no host!"})
			return
		}
		// http://mygo
		rsp, err := http.Get("http://" + host)
		if err != nil {
			context.JSON(400, gin.H{"error": err})
		} else {
			b, err := ioutil.ReadAll(rsp.Body)
			if err != nil {
				context.JSON(400, gin.H{"error": err})
			} else {
				context.JSON(200, gin.H{"message": string(b)})
			}

		}
	})

	err := r.Run(":80")
	if err != nil {
		log.Fatal(err)
	}
}

```

主要是测试转发集群ip的内容

参考步骤5 需要hostport进行测试

![image-20220703191155911](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207031911326.png)

测试访问转发向clusterip

![image-20220703191712431](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207031917324.png)

## 8、nfs的挂载部署

安装nfs

```sh
yum -y install nfs-utils 
```

```sh
vi /etc/sysconfig/nfs #修改配置
```

```sh
LOCKD_TCPPORT=30001 #TCP锁使用端口
LOCKD_UDPPORT=30002 #UDP锁使用端口
MOUNTD_PORT=30003 #挂载使用端口
STATD_PORT=30004 #状态使用端口
```

```sh
vi /etc/exports #添加挂载
```

```
/root/goapp    192.168.2.0/24(rw,async,insecure,no_root_squash)
```

root_squash（默认）：将来访的root用户映射为匿名用户或用户组；

no_root_squash：来访的root用户保持root帐号权限 ；

no_all_squash（默认）：访问用户先与本机用户匹配，匹配失败后再映射为匿名用户或用户组；

all_squash：将来访的所有用户映射为匿名用户或用户组；

secure（默认）：限制客户端只能从小于1024的tcp/ip端口连接服务器；insecure：允许客户端从大于1024的tcp/ip端口连接服务器；

anonuid：匿名用户的UID值，通常是nobody或nfsnobody，可以在此处自行设定；

anongid：匿名用户的GID值；

no_subtree_check：如果NFS输出的是一个子目录，则无需检查其父目录的权限（可以提高效率） 



启动

```sh
sudo systemctl restart rpcbind.service 
sudo systemctl restart nfs-server.service
# 开机启动：
sudo  systemctl enable rpcbind.service
sudo systemctl enable nfs-server.service

```

测试

```
 showmount -e 192.168.2.3
```



创建pv pvc

![image-20220703213132776](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207032131697.png)

![image-20220703213500981](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207032135321.png)

修改nfs挂载

![image-20220703213741415](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207032137834.png)

## 9、ingress的路由重写

```yaml
nginx.ingress.kubernetes.io/rewrite-target: /$1
```

需要对ingress 加一层重写/api/+路由

![image-20220703215301458](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207032153696.png)

![image-20220703215312305](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207032153463.png)

测试

![image-20220703215204858](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207032152193.png)

## 10、ingress的证书上传

上传对应的证书

![image-20220703220537092](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207032205160.png)

ingress 选择使用的证书  和 域名

![image-20220703220640659](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207032206104.png)

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  creationTimestamp: "2022-07-16T10:05:28Z"
  generation: 1
  managedFields:
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:rules: {}
        f:tls: {}
    manager: agent
    operation: Update
    time: "2022-07-16T10:05:28Z"
  name: test123
  namespace: default
  resourceVersion: "1183026"
  uid: 84f32151-006e-493a-af20-88a350ed3e91
spec:
  ingressClassName: nginx
  rules:
  - host: test.xiongmao.com
    http:
      paths:
      - backend:
          service:
            name: registry
            port:
              number: 5000
        path: /
        pathType: Prefix
  tls:
  - hosts:
    - test.xiongmao.com
    secretName: ssl
```



## 11、ingress配置websocket



```go
package main

import (
	"flag"
	"fmt"
	"github.com/gorilla/websocket"
	"log"
	"net/http"
)

var addr = flag.String("addr", ":8081", "http service address")

var upgrader = websocket.Upgrader{CheckOrigin: func(r *http.Request) bool {
	return true
}}

func echo(w http.ResponseWriter, r *http.Request) {
	c, err := upgrader.Upgrade(w, r, nil)
	if err != nil {
		log.Println("upgrader", err)
		return
	}
	defer c.Close()
	for {
		mt, message, err := c.ReadMessage()
		if err != nil {
			log.Println("read", err)
			break
		}
		log.Println("recv: ", fmt.Sprintf("%s", message))
		err = c.WriteMessage(mt, message)
		if err != nil {
			log.Println("write", err)
			break
		}
	}
}
func main() {
	http.HandleFunc("/echo", echo)
	log.Fatal(http.ListenAndServe(*addr, nil))
}

```

参看步骤5进行部署

ingress映射

![image-20220704001252640](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207040012701.png)



![image-20220704001414012](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207040014189.png)



测试结果

![image-20220704010336967](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207040103140.png)

## 12、rancher安装redis

```
mkdir -p /root/redis/n1/{conf,data,logs}
```

```
vim redis.conf
```

```
daemonize no 
port 6379
bind 0.0.0.0
logfile /logs/redis.log   
dir /data  
```

添加存储nfs路径

```
vi /etc/exports
```

```
/root/redis    192.168.2.0/24(rw,async,insecure,no_root_squash)
```

```
exportfs -a  
```

添加pv、pvc

![image-20220704021111516](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207040211913.png)

![image-20220704021214437](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207040212525.png)

部署redis服务

```sh
 redis-server /conf/redis.conf
 #启动命令
```

![image-20220704022940685](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207040229333.png)

添加nfs映射

![image-20220704023011623](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207040230757.png)

## 13、go对集群redis调用



```go
package main

import (
	"github.com/gin-gonic/gin"
	"github.com/go-redis/redis"
)

func main() {
	rdb := redis.NewClient(&redis.Options{
		Addr:     "myredis:6379", // 指定集群svc
		Password: "",
		DB:       0, // redis一共16个库，指定其中一个库即可
	})
	r := gin.New()
    // 自定义函数
	r.Handle("GET", "/", func(context *gin.Context) {
        // 获取请求的key
		key := context.Query("key")
        // redis获取key，给gin返回
		ret, err := rdb.Get(key).Result()
		if err != nil {
			context.String(400, err.Error())
		} else {
			context.String(200, ret)
		}
	})
	r.Run(":80")

}

```



参考步骤5部署，实现对集群内myredis的访问

![image-20220704112640838](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207041126594.png)

## 14、configmap挂载redis配置

创建configmap，redis-config的配置

![image-20220704152153377](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207041521633.png)



引用configmap

![image-20220706074258247](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207060743195.png)

## 15、rancher搭建registry

主要以docker registry进行实验

官方地址：https://hub.docker.com/_/registry



```
mkdir -p /root/registry/data
vim /root/registry/dconfigmap.yml
```

```yaml
version: 0.1
log:
 fields:
 service: registry
storage:
 delete:
  enabled: true
 cache:
  blobdescriptor: inmemory
 filesystem:
  rootdirectory: /var/lib/registry
http:
 addr: :5000
 headers:
  X-Content-Type-Options: [nosniff]
health:
 storagedriver:
 enabled: true
 interval: 10s
 threshold: 3
```

 





![image-20220707231237891](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207072314944.png)



![image-20220707231313032](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207072313317.png)





根据路径映射

```
/root/registry/:/etc/docker/registry/
/root/registry/data:/var/lib/registry                                                                                                  
```

![image-20220707231428016](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207072314055.png)

测试docker 推送镜像



```sh
vim /etc/docker/daemon.json #修改docker
```

```
{
    "insecure-registries":["192.168.2.3:5000"]
}
```

```sh
systemctl  reload docker #然后重载docker 即可
```





```sh
docker tag  busybox:latest  192.168.2.3:5000/busybox:latest
docker push 192.168.2.3:5000/busybox:latest
```

## 16、rancher搭建gitlab

```sh
vim /etc/export
```

```sh
/root/gitlab    192.168.2.0/24(rw,async,insecure,no_root_squash)
# 新增此行
```

```
 exportfs -a
```

参考前面将pv、pvc部署后，部署服务

```sh
#映射对应的路径
/etc/gitlab  logs
/var/opt/gitlab  data
/var/log/gitlab config
```

![1](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207091019993.gif)



![image-20220709102829253](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207091028875.png)

测试结果

![image-20220709103136053](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207091031235.png)



```sh
#账号 密码
root
/etc/gitlab/initial_root_password  /root/gitlab/logs
```



## 17、简单goland的cicd的实现

### 17.1、修改gitlab的https推送地址

![1](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207091141413.gif)

添加账号后、添加ssh-key

![image-20220709115030729](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207091150762.png)



添加gitlab的ssh端口 其中为30022

![image-20220709115241609](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207091152601.png)

```sh
ssh -T git@git.xiongmao.com -p 30022 #本机对gitlab的sshkey是否可以使用，进行测试
```



### 17.2、将goland准备上传至gitlab

在goland中执行

```sh
git init 
git remote -v #多个仓库的话，进行查看
git remote add origin2 https://gitlab.xiongmao.com/xiongmao/mygo.git #指定一个新的仓库进行上传
git remote set-url origin2 https://gitlab.xiongmao.com/xiongmao/mygo.git
git config --global --list
git config --global user.name "xiongmao"
git config --global user.email "498510210@qq.com"
```

```
ssh://git@git.xiongmao.com:30022/xiongmao/mygo.git
```

```sh
git pull origin2 main --allow-unrelated-histories #拉取初始化，防止冲突
```

测试

![1](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207091456828.gif)

## 18、gitlab-ci的实现

### 18.1、gitlab-runner的启动

生成gitlab-runner的配置文件

```sh
 docker run -d --name gitlab-runner   \
  -v /root/gitlab-runner:/etc/gitlab-runner \
  -v /var/run/docker.sock:/var/run/docker.sock \
 -v /usr/bin/docker:/usr/bin/docker \
 -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7  \
  -v /etc/hosts:/etc/hosts  \
  gitlab/gitlab-runner
   #或者  --add-host="gitlab.xiongmao.com:192.168.2.2" 
```

```sh
chmod 666 /var/run/docker.sock
```

实现对gitlab的注册

```sh
docker exec -it gitlab-runner  gitlab-runner register
```

![1](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207092153368.gif)

```sh
docker restart gitlab-runner
```

重新启动

生成了其中的配置文件

```sh
vim /root/gitlab-runner/config.toml
```

```yaml
concurrent = 1
check_interval = 0

[session_server]
  session_timeout = 1800

[[runners]]
  name = "mygo desc"
  url = "http://gitlab.xiongmao.com/"
  token = "758efDhs73dNi7-5Vwvg"
  executor = "shell"
  [runners.custom_build_dir]
  [runners.cache]
    [runners.cache.s3]
    [runners.cache.gcs]
    [runners.cache.azure]
```

### 18.2、gitlab-ci yaml的简单创建

goland中编辑所在项目gitlab-ci的yaml文件

```
vim 
```

```sh
variables:
  GIT_SSL_NO_VERIFY: "1"
stages:
  - test
  - build
job1:
  stage: test
  script:
    - echo "it is test"
  tags:
    - go

job2:
  stage: build
  script:
    - echo "it is build"
  tags:
    - go
```

其中各个字段含义

- script             由Runner执行的Shell脚本。

- image              使用docker镜像，  image：name

- service            使用docker  services镜像, services：name

- before_script      执行作业前运行的脚本

- after_script       作业完成后运行的脚本

- stages             定义管道中的步骤，依次运行

- stage              定义管道中步骤的作业段

- only    　　        指定作业限制only:refs，only:kubernetes，only:variables，和only:changes

- tags               指定执行作业的runner

- allow_failure      允许job失败

- when               什么时候开始工作，

    on_success       只有当前一个阶段的所有工作都成功时（或者因为它们被标记而被认为是成功的allow_failure）才执行工作 。这是默认值。

    on_failure       仅当前一阶段的至少一个作业失败时才执行作业。

    always           无论先前阶段的工作状态如何，都可以执行工作。

    manual           手动执行作业

    delayed          延迟作业。后面跟start_in,start_in 30minutes(延迟30分钟)，不加单位，默认为秒。最长可延迟1小时。

   environment     作业部署到的环境名称   #暂未搞清

-  cache    

    　key："$CI_JOB_STAGE-$CI_COMMIT_REF_SLUG" #为每分支，每步骤启用缓存

- artifacts         job成功时附加到作业的文件或目录

- dependencies      此job依赖其他jobz,主要作用于作业优先级

- converage         给定作业代码覆盖率设置　　　　　　 

- retry             在发生故障时，可以自动重试作业的次数。

- parallel　　      应该并行运行多少个作业实例

- trigger          定义下游管道触发器

- include          允许此作业包含外部YAML

- extends          此作业将继承的配置项

- pages            上传作业结果用于gitlab pages

- variables        作业级别定义作业变量



参考步骤17进行推送

![image-20220709230333053](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207092304638.png)

### 18.3、shell模块的镜像编译

gitlab的shell模块build镜像

```sh
variables:
  GIT_SSL_NO_VERIFY: "1"
stages:
  - build
job1:
  stage: build
  script:
    - docker build -t mygo:v1  .
  tags:
    - go
```

添加Dockerfile

```SH
FROM golang:1.14.4-alpine3.12
RUN mkdir /src /app

ADD . ../src
ENV GOPROXY="https://goproxy.io"
RUN cd /src && ls && go build -o ../app/mygo hello.go && cd /app && chmod +x mygo && cd /
RUN rm src -fr
WORKDIR /app
ENTRYPOINT  ["/app/mygo"]
```

查看build镜像

![image-20220710010128561](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207100101845.png)

Dockerfile修改启动镜像

```yaml
FROM golang:1.14.4-alpine3.12
RUN mkdir /src /app

ADD . ../src
ENV GOPROXY="https://goproxy.io"
RUN cd /src && ls && go build -o ../app/mygo hello.go && cd /app && chmod +x mygo && cd /
#编译容器

FROM alpine:3.12
RUN mkdir /app
COPY --from=0 /app/mygo /app
#运行容器
ENTRYPOINT ["/app/mygo"]

```

### 18.4 、gitlab-ci单元测试

修改gitlab-ci测试

```sh
variables:
  GIT_SSL_NO_VERIFY: "1"
stages:
  - test
  - build
GoTest:
  stage: test
  script:
    - docker build -f DockerfileTest -t test-mygo:v1 .
    # 指定Dockerfile名称进行test
    - docker run --rm test-mygo:v1
  after_script:
    - docker rmi test-mygo:v1
  tags:
    - go

GoBuild:
  stage: build
  script:
    - docker build -t mygo:v1  .
  after_script:
    - docker rmi $(docker images -af "dangling=true" -q)
    # 清理额外的镜像
  tags:
    - go

```

主程序

```sh
package main

import (
	"fmt"
	"regexp"
)

func isAllNumber(str string) bool {
	reg := regexp.MustCompile(`^\d+$`)
	return reg.MatchString(str)
}

func main() {
	fmt.Println("123456")
}

```

单元测试

```sh
package main

import "testing"

func Test_isAllNumber(t *testing.T) {
	tests := []struct {
		name string
		args string
		want bool
	}{
		// TODO: Add test cases.
		{"t1", "abc", false},
		{"t2", "a123", false},
		{"t3", "1_23", false},
		{"t4", "123", true},
		{"t5", "1234a", false},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := isAllNumber(tt.args); got != tt.want {
				t.Errorf("isAllNumber() = %v, want %v", got, tt.want)
			}
		})
	}
}

```



```sh
# DockerfileTest
FROM golang:1.14.4-alpine3.12
ADD . /src
WORKDIR /src
cmd ["go","test"]
# 执行测试，如果exit为1则单元测试不通过
```

```sh
# Dockerfile
# 单元测试后，正常编译
FROM golang:1.14.4-alpine3.12
RUN mkdir /src

ADD . /src
ENV GOPROXY="https://goproxy.io"
RUN cd /src && ls && go build -o mygo main.go  && chmod +x mygo

FROM alpine:3.12
RUN mkdir /app
COPY --from=0 /src/mygo /app
ENTRYPOINT  ["/app/mygo"]
```

### 18.5、推送镜像

goland-上传gitlab-gitlabrunner拉取执行shell-上传镜像仓库的ci

```yaml
variables:
  GIT_SSL_NO_VERIFY: "1"
stages:
  - test
  - build
  - deploy
GoTest:
  stage: test
  script:
    - docker build -f DockerfileTest -t test-mygo:v1 .
    - docker run --rm test-mygo:v1
  after_script:
    - docker rmi test-mygo:v1
  tags:
    - go

GoBuild:
  stage: build
  script:
    - docker build -t mygo:v1  .
  after_script:
    - docker rmi $(docker images -af "dangling=true" -q)
  tags:
    - go
# 新增部署
GoDeploy:
  stage: deploy
  script:
    - docker tag  mygo:v1  192.168.2.3:5000/mygo:v1
    - docker push 192.168.2.3:5000/mygo:v1
  after_script:
    - docker rmi 192.168.2.3:5000/mygo:v1
    - docker rmi mygo:v1
  tags:
    - go

```

go test的dockefile

```sh
FROM golang:1.17.11-alpine3.16
ADD . /src
WORKDIR /src
ENV GOPROXY=https://goproxy.cn
RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories
# 修改阿里云的源
RUN apk add build-base
cmd ["go","test"]
```

dockerfile

```sh
FROM golang:1.17.11-alpine3.16
RUN mkdir /src
RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories
RUN apk add build-base
ADD . /src
RUN cd /src && ls && GOPROXY=https://goproxy.cn go build -o mygo main.go  && chmod +x mygo


FROM alpine:3.12
RUN mkdir /app
COPY --from=0 /src/mygo /app
ENTRYPOINT  ["/app/mygo"]
```

测试代码

```go
package main

import "testing"

func Test_httpGet(t *testing.T) {
    go main()
	tests := []struct {
		name string
		args string
		want string
	}{
		{"t1", "http://127.0.0.1", "你好，胸毛"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := httpGet(tt.args); got != tt.want {
				t.Errorf("httpGet() = %v, want %v", got, tt.want)
			}
		})
	}
}

```

代码

```go
package main

import (
	"github.com/gin-gonic/gin"
	"io/ioutil"
	"log"
	"net/http"
)

func httpGet(url string) string {
	response, err := http.Get(url)
	if err != nil {
		log.Println("get error")
	}
	defer response.Body.Close()
	body, err2 := ioutil.ReadAll(response.Body)
	if err2 != nil {
		log.Println("ioutil read error")
	}
	return string(body)
}

func main() {
	//Default返回一个默认的路由引擎
	r := gin.Default()
	r.Handle("GET", "/", func(c *gin.Context) {
		c.String(200, "你好，胸毛")
	})
	r.Run(":80")
}

```

```sh
curl http://192.168.2.3:5000/v2/_catalog #  (查看列表)
curl http://192.168.2.3:5000/v2/mygo/manifests/v1   #(#查看redis镜像详情)
curl http://192.168.2.3:5000/v2/mygo/tags/list #版本
```

### 18.6、发布服务

goland-上传gitlab-gitlabrunner拉取执行shell-上传镜像仓库的ci-发布服务到rancher

runner对集群的控制

```sh
docker cp 79f7a7e15704:/usr/bin/kubectl . #从rancher容器中拷贝命令
```

```
docker rm -f gitlab-runner
```

```sh
 # 指定集群证书文件
 docker run -d --name gitlab-runner   \
  -v /root/gitlab-runner:/etc/gitlab-runner \
  -v /var/run/docker.sock:/var/run/docker.sock \
 -v /usr/bin/docker:/usr/bin/docker \
 -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7  \
  -v /etc/hosts:/etc/hosts  \
  -v /usr/local/bin/kubectl:/usr/local/bin/kubectl \
-v /root/kubectlconfig/config:/kubeconfig \
-e KUBECONFIG=/kubeconfig/config \
  gitlab/gitlab-runner
   #或者  --add-host="gitlab.xiongmao.com:192.168.2.2"  
```

新增ranncher的部署

```sh
variables:
  GIT_SSL_NO_VERIFY: "1"
stages:
  - test
  - build
  - deploy
  - publish
GoTest:
  stage: test
  script:
    - docker build -f DockerfileTest -t test-mygo:v1 .
    - docker run --rm test-mygo:v1
  after_script:
    - docker rmi test-mygo:v1
  tags:
    - go

GoBuild:
  stage: build
  script:
    - docker build -t mygo:v1  .
  after_script:
    - docker rmi $(docker images -af "dangling=true" -q)
  tags:
    - go

GoDeploy:
  stage: deploy
  script:
    - docker tag  mygo:v1  192.168.2.3:5000/mygo:v1
    - docker push 192.168.2.3:5000/mygo:v1
  after_script:
    - docker rmi 192.168.2.3:5000/mygo:v1
    - docker rmi mygo:v1
  tags:
    - go
GoPub:
  stage: publish
  script:
    - kubectl get pod | grep mygo | awk '{print $1}'  | xargs kubectl delete pod
  tags:
    - go

```

正常服务

![image-20220710141222310](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207101412567.png)

![image-20220710142744064](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207101427141.png)

## 19、金丝雀发布(灰度发布)

### 19.1、基于头的灰度发布

需要部署2个版本的服务，ingress实现金丝雀发布

![image-20220710180147473](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207101801270.png)

![image-20220710180643212](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207101806507.png)





对后端容器负载

设置

![image-20220710182956257](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207101829580.png)

正常的

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    field.cattle.io/publicEndpoints: '[{"addresses":["192.168.2.2"],"port":80,"protocol":"HTTP","serviceName":"default:mygo","ingressName":"default:mygo","hostname":"mygo.xiongmao.com","path":"/","allNodes":true}]'
  creationTimestamp: "2022-07-10T10:00:51Z"
  generation: 4
  managedFields:
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:loadBalancer:
          f:ingress: {}
    manager: nginx-ingress-controller
    operation: Update
    subresource: status
    time: "2022-07-10T10:01:15Z"
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:field.cattle.io/publicEndpoints: {}
      f:spec:
        f:rules: {}
    manager: agent
    operation: Update
    time: "2022-07-10T10:13:28Z"
  name: mygo
  namespace: default
  resourceVersion: "1557485"
  uid: 20e5ad39-9eb3-41fa-bb79-75e2cfc69682
spec:
  ingressClassName: nginx
  rules:
  - host: mygo.xiongmao.com
    http:
      paths:
      - backend:
          service:
            name: mygo
            port:
              number: 80
        path: /
        pathType: Prefix
status:
  loadBalancer:
    ingress:
    - ip: 192.168.2.2
    - ip: 192.168.2.3

```

金丝雀

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    field.cattle.io/publicEndpoints: '[{"addresses":["192.168.2.2"],"port":80,"protocol":"HTTP","serviceName":"default:mygo2","ingressName":"default:mygo2","hostname":"mygo.xiongmao.com","path":"/","allNodes":true}]'
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-by-header: v
    nginx.ingress.kubernetes.io/canary-by-header-value: "2"
  creationTimestamp: "2022-07-10T10:22:30Z"
  generation: 5
  managedFields:
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:loadBalancer:
          f:ingress: {}
    manager: nginx-ingress-controller
    operation: Update
    subresource: status
    time: "2022-07-10T10:23:15Z"
  - apiVersion: networking.k8s.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:field.cattle.io/publicEndpoints: {}
          f:nginx.ingress.kubernetes.io/canary: {}
          f:nginx.ingress.kubernetes.io/canary-by-header: {}
          f:nginx.ingress.kubernetes.io/canary-by-header-value: {}
      f:spec:
        f:rules: {}
    manager: agent
    operation: Update
    time: "2022-07-10T10:24:07Z"
  name: mygo2
  namespace: default
  resourceVersion: "1557192"
  uid: a72eb0e9-95fd-41bc-a706-6396ece94885
spec:
  ingressClassName: nginx
  rules:
  - host: mygo.xiongmao.com
    http:
      paths:
      - backend:
          service:
            name: mygo2
            port:
              number: 80
        path: /
        pathType: Prefix
status:
  loadBalancer:
    ingress:
    - ip: 192.168.2.2
    - ip: 192.168.2.3
```



```sh
nginx.ingress.kubernetes.io/canary
nginx.ingress.kubernetes.io/canary-by-header
nginx.ingress.kubernetes.io/canary-by-header-value
# 流量
nginx.ingress.kubernetes.io/canary-weight
nginx.ingress.kubernetes.io/canary-by-cookie
```

测试结果，根据头部进行访问

![image-20220710183036614](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207101830979.png)

![image-20220710183058710](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207101830747.png)

### 19.2、基于流量的灰度发布

修改流量的权重为50

![image-20220710183736699](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207101837739.png)



测试结果

```go
package main

import (
	"fmt"
	"io/ioutil"
	"log"
	"net/http"
)

func httpGetTest(url string) {
	response, err := http.Get(url)
	if err != nil {
		log.Println("get error")
	}
	defer response.Body.Close()
	body, err2 := ioutil.ReadAll(response.Body)
	if err2 != nil {
		log.Println("ioutil read error")
	}
	fmt.Println(string(body))
}
func main() {
	for i := 0; i < 10; i++ {
		httpGetTest("http://mygo.xiongmao.com")
	}
}

```

![image-20220711102601675](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207111026536.png)

### 19.3、基于K8S的CICD

修改gitlab-ci

```yaml
variables:
  GIT_SSL_NO_VERIFY: "1"
stages:
  - test
  - build
  - deploy
  - publish
GoTest:
  stage: test
  script:
    - docker build -f DockerfileTest -t test-mygo:v1 .
    - docker run --rm test-mygo:v1
  after_script:
    - docker rmi test-mygo:v1
  tags:
    - go

GoBuild:
  stage: build
  script:
    - docker build -t mygo:v2  .
  after_script:
    - docker rmi $(docker images -af "dangling=true" -q)
  tags:
    - go

GoDeploy:
  stage: deploy
  script:
    - docker tag  mygo:v2  192.168.2.3:5000/mygo:v2
    - docker push 192.168.2.3:5000/mygo:v2
  after_script:
    - docker rmi 192.168.2.3:5000/mygo:v2
    - docker rmi mygo:v2
  tags:
    - go
GoPub:
  stage: publish
  script:
    - kubectl apply -f  mygo.yaml
  tags:
    - go

```



部署服务

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mygo2
  namespace: default
spec:
  selector:
    matchLabels:
      app: mgo
  replicas: 1
  template:
    metadata:
      labels:
        app: mgo
    spec:
      containers:
        - name: mygo
          image: 192.168.2.3:5000/mygo:v2
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: mygo2
  namespace: default
spec:
  selector:
    app: mgo
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "30"
    nginx.ingress.kubernetes.io/canary-by-header: "v"
    nginx.ingress.kubernetes.io/canary-by-header-value: "2"
  name: mygo2
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: mygo.xiongmao.com
    http:
      paths:
      - backend:
          service:
            name: mygo2
            port:
              number: 80
        path: /
        pathType: Prefix        
```

## 20、ETCD集群部署

### 20.1、部署服务

官方地址https://quay.io/repository/coreos/etcd?tag=latest&tab=tags

```sh
  docker pull  quay.mirrors.ustc.edu.cn/coreos/etcd:v3.3.25 #代理地址
  docker tag quay.mirrors.ustc.edu.cn/coreos/etcd:v3.3.25 etcd:3.3.25
  docker rmi quay.mirrors.ustc.edu.cn/coreos/etcd:v3.3.25
```

创建nfs的目录

```
mkdir -p   /root/etcdconf/data
```

执行挂载

```
vim /etc/exports
```

```sh
/root/goapp    192.168.2.0/24(rw,async,insecure,no_root_squash)
/root/redis    192.168.2.0/24(rw,async,insecure,no_root_squash)
/root/gitlab    192.168.2.0/24(rw,async,insecure,no_root_squash)
/root/etcdconf  192.168.2.0/24(rw,async,insecure,no_root_squash)
```

```
exportfs -a
```

创建pv、pvc

![image-20220712012644238](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207120126980.png)

![image-20220712012710286](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207120127338.png)

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/bound-by-controller: "yes"
  creationTimestamp: "2022-07-11T17:26:31Z"
  finalizers:
  - kubernetes.io/pv-protection
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:accessModes: {}
        f:capacity:
          .: {}
          f:storage: {}
        f:nfs:
          .: {}
          f:path: {}
          f:server: {}
        f:persistentVolumeReclaimPolicy: {}
        f:volumeMode: {}
    manager: agent
    operation: Update
    time: "2022-07-11T17:26:31Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:phase: {}
    manager: kube-controller-manager
    operation: Update
    subresource: status
    time: "2022-07-11T17:26:31Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:pv.kubernetes.io/bound-by-controller: {}
      f:spec:
        f:claimRef: {}
    manager: kube-controller-manager
    operation: Update
    time: "2022-07-11T17:28:01Z"
  name: etcdpv
  resourceVersion: "192864"
  uid: 3193923b-f344-4c43-ab7f-a0c3dd21dcfc
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 10Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: etcdpvc
    namespace: default
    resourceVersion: "192862"
    uid: f6f3eddc-9e23-4ec8-915d-ab7c7c30d763
  nfs:
    path: /root/etcdconf
    server: 192.168.2.3
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem
status:
  phase: Bound

```



```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    pv.kubernetes.io/bind-completed: "yes"
  creationTimestamp: "2022-07-11T17:28:01Z"
  finalizers:
  - kubernetes.io/pvc-protection
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:accessModes: {}
        f:resources:
          f:requests:
            .: {}
            f:storage: {}
        f:storageClassName: {}
        f:volumeMode: {}
        f:volumeName: {}
    manager: agent
    operation: Update
    time: "2022-07-11T17:28:01Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:pv.kubernetes.io/bind-completed: {}
    manager: kube-controller-manager
    operation: Update
    time: "2022-07-11T17:28:01Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:accessModes: {}
        f:capacity:
          .: {}
          f:storage: {}
        f:phase: {}
    manager: kube-controller-manager
    operation: Update
    subresource: status
    time: "2022-07-11T17:28:01Z"
  name: etcdpvc
  namespace: default
  resourceVersion: "192866"
  uid: f6f3eddc-9e23-4ec8-915d-ab7c7c30d763
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: ""
  volumeMode: Filesystem
  volumeName: etcdpv
status:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 10Gi
  phase: Bound

```

创建cm

```
name: etcd1
data_dir: /etcd/data
listen-client-urls: http://0.0.0.0:2379
```

![image-20220712021127720](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207120256687.png)

部署服务

![1](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207120224856.gif)

### 20.2、测试连接

客户端库[https://](https://github.com/etcd-io/etcd/tree/master/clientv3)[github.com/etcd-io/etcd/tree/master/clientv3](https://github.com/etcd-io/etcd/tree/master/clientv3)

降低依赖库

```
replace google.golang.org/grpc => google.golang.org/grpc v1.26.0 
```

```go
package main

import (
	"context"
	"github.com/coreos/etcd/clientv3"
	"log"

	"time"
)

func main() {
	cli, err := clientv3.New(clientv3.Config{
		Endpoints:   []string{"192.168.2.2:2379"},
		DialTimeout: 5 * time.Second,
	})
	if err != nil {
		log.Fatal(err)
	}
	defer cli.Close()
	kv := clientv3.NewKV(cli)
	ctx := context.Background()
	_, err = kv.Put(ctx, "/service/test", "testservice")
	if err != nil {
		log.Fatal(err)
	}

}

```

进入容器测试

```
export ETCDCTL_API=3  #切换为API3
etcdctl get /service/test 
```

![image-20220713230804442](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207132308184.png)

查看是否有新版本

![image-20220714000739593](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207140007550.png)

### 20.3、etcd 集群部署

```
name: etcd1
data-dir: /etcd/data
listen-client-urls: http://0.0.0.0:2379
advertise-client-urls: http://0.0.0.0:2379
listen-peer-urls: http://0.0.0.0:2380
initial-advertise-peer-urls: http://etcd1:2380
initial-cluster-token: 'etcd-cluster'
initial-cluster: etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380
initial-cluster-state: 'new'
```

```
name: etcd2
data-dir: /etcd/data
listen-client-urls: http://0.0.0.0:2379
advertise-client-urls: http://0.0.0.0:2379
listen-peer-urls: http://0.0.0.0:2380
initial-advertise-peer-urls: http://etcd2:2380
initial-cluster-token: 'etcd-cluster'
initial-cluster: etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380
initial-cluster-state: 'new'
```

```
name: etcd3
data-dir: /etcd/data
listen-client-urls: http://0.0.0.0:2379
advertise-client-urls: http://0.0.0.0:2379
listen-peer-urls: http://0.0.0.0:2380
initial-advertise-peer-urls: http://etcd3:2380
initial-cluster-token: 'etcd-cluster'
initial-cluster: etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380
initial-cluster-state: 'new'
```

![image-20220714014633744](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207140146928.png)

部署3个不同的配置

![image-20220714012550630](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207140146461.png)

修改环境变量

![image-20220714013922726](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207140146733.png)

```sh
#查看节点列表
etcdctl -w table member list

#查看单节点信息
 etcdctl -w table  endpoint status

#查看全部
 etcdctl -w table --endpoints=etcd1:2379,etcd2:2379,etcd3:2379  endpoint status

```

## 21、sidecar采集日志示例

安装  [https://](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.html)[www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.html](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.html)

配置 [https://](https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html)[www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html](https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html)

创建configmap

```sh
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/nginx/access.log
output.console:
  pretty: true
setup.template.enabled: false
scan_frequency: 2s
```



创建边车

```sh
docker.elastic.co/beats/filebeat:7.10.0
```

```sh
filebeat  -e -c /config/filebeat.yaml
```



rancher 不支持可视化创建临时卷，编辑yaml创建临时卷

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "6"
  creationTimestamp: "2022-07-16T02:28:51Z"
  generation: 6
  labels:
    workload.user.cattle.io/workloadselector: apps.deployment-default-nginx
  - apiVersion: apps/v1
  name: nginx
  namespace: default
  resourceVersion: "1121230"
  uid: c13f82b9-8b8b-4594-97df-b7b2dd5aa835
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      workload.user.cattle.io/workloadselector: apps.deployment-default-nginx
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        cattle.io/timestamp: "2022-07-16T03:12:07Z"
      creationTimestamp: null
      labels:
        workload.user.cattle.io/workloadselector: apps.deployment-default-nginx
    spec:
      affinity: {}
      containers:
      - image: nginx:latest
        imagePullPolicy: IfNotPresent
        name: container-0
        ports:
        - containerPort: 80
          name: nginx
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        # 引入临时卷
        - mountPath: /var/log/nginx
          name: nginx
      - command:
        - filebeat
        - -e
        - -c
        - /config/filebeat.yaml
        image: docker.elastic.co/beats/filebeat:7.10.0
        imagePullPolicy: IfNotPresent
        name: container-2
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /config/filebeat.yaml
          name: vol-g4ni9
          subPath: filebeat
        # 引入临时卷
        - mountPath: /var/log/nginx
          name: nginx
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          defaultMode: 429
          name: filebeat
        name: vol-g4ni9
      # 添加临时卷
      - emptyDir: {}    
        name: nginx
```



![1](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161126656.gif)

访问测试访问nginx后，测试容器是否有标准输出

![image-20220716111845930](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161126131.png)

## 22、部署elk

安装  https://www.elastic.co/guide/en/elasticsearch/reference/7.4/docker.html

```sh
sysctl -w vm.max_map_count=262144 #设置最大交换内存
```

### 22.1、单节点搭建

创建configmap

```sh
/usr/share/elasticsearch/config/elasticsearch.yml  #设置路径
```

![image-20220716121719894](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161217090.png)



```sh
docker.elastic.co/elasticsearch/elasticsearch:7.4.2 #启动镜像版本
```

```yaml
# configmap配置
cluster.name: myes
network.host: 0.0.0.0
http.port: 9200
node.name: "es1"
cluster.initial_master_nodes: ["es1"]
```

### 22.2、集群配置

![image-20220716132613617](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161326861.png)

增加集群配置

```yaml
transport.tcp.port: 9300
discovery.zen.ping.unicast.hosts: ["es1", "es2"]
```

增加es服务

![image-20220716124848108](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161250470.png)

验证集群可用性

```
curl 127.0.0.1:9200/_cat/nodes
```

![image-20220716132738675](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161327716.png)

### 22.3、设置kibana

安装文档 https://www.elastic.co/guide/en/kibana/current/docker.html

版本信息  docker.elastic.co/kibana/kibana:7.4.2  

镜像加速  [https://](https://hub.docker.com/r/elastic/kibana/tags)[hub.docker.com/r/elastic/kibana/tags](https://hub.docker.com/r/elastic/kibana/tags)

文件目录 /usr/share/kibana/config/kibana.yml

创建配置文件

```yaml
server.port: 5601   
server.host: "0.0.0.0"
 
#ES请求的服务URL
elasticsearch.hosts: ["http://es1:9200","http://es2:9200"]
#无证书
elasticsearch.ssl.verificationMode: none
xpack.security.enabled: false
#不使用安全验证

```

测试连接情况

![image-20220716140042674](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161400308.png)

### 22.4、基于ingress的kibana Basic Auth身份验证

文档 [https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#authentication](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/)



```sh
yum -y install httpd-tools
@Apache的Web服务器内置的工具,用于创建和更新储存用户名和用户基本认证的密码文件

#创建一个密码文件  
htpasswd -c auth xiongmao
@这时产生了一个 auth文件

#再添加一个  用户(此时不用-c参数)
htpasswd  auth lisi

```

将生成的密吗创建secret

```
xiongmao:$apr1$ymQufzyZ$ya8mrJ2JAOHlPbPG.J2CI1
lisi:$apr1$JoZ0Umoz$qIJQSOqylL9NMgGqfIQjU/
```

![image-20220716142647420](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161432206.png)

创建ingress，设置认证

```
nginx.ingress.kubernetes.io/auth-type: basic
nginx.ingress.kubernetes.io/auth-secret: kb-auth
```

![image-20220716142927044](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161432166.png)

测试访问

![image-20220716143234241](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161432602.png)

### 22.5、es的中文分词插件es

下载官方 https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.4.2

插件路径 /usr/share/elasticsearch/plugins

```sh
mkdir /root/es/plugins -p
vim /etc/exports #nfs
```

```sh
/root/es    192.168.2.0/24(rw,async,insecure,no_root_squash)
# 加入配置
```

```sh
 exportfs -a #重载
```

下载上传插件至nfs

```sh
useradd elasticsearch
chown -R elasticsearch:elasticsearch analysis-ik/ -R
```

修改服务映射

![image-20220716151058712](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161511366.png)

测试

```
POST  _analyze
{
  "analyzer": "ik_smart",
   "text" : "他们在学go"
}
```

![image-20220716153104820](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161531012.png)

### 22.6、sql访问

[文档](https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-getting-started.html)[ ](https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-getting-started.html) [https](https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-getting-started.html)[://](https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-getting-started.html)[www.elastic.co/guide/en/elasticsearch/reference/current/sql-getting-started.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-getting-started.html)

https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-syntax-select.html

创建索引

```json
 PUT /books
{
  "mappings": {
    "properties": {
      "BookID":    { "type": "integer" },
      "BookName":    { "type": "text","analyzer": "ik_max_word","search_analyzer": "ik_smart","fields":{ "keyword":{"type":"keyword","ignore_above":256}}},  
      "BookPrice":   { "type": "float"},  
      "BookAuthor":   { "type": "keyword"}
    }
  }
}

```

插入数据

```yaml
POST _bulk
{ "index" : { "_index" : "books", "_id" : "101" } }
{"BookID":101,"BookName":"C语言程序设计","BookPrice":19,"BookAuthor":"老蒋"}
{ "index" : { "_index" : "books", "_id" : "102" } }
 {"BookID":102,"BookName":"PHP高级编程","BookPrice":29,"BookAuthor":"老李"}
{ "index" : { "_index" : "books", "_id" : "103" } }
 {"BookID":103,"BookName":"java编程从入门到精通","BookPrice":39,"BookAuthor":"老王"}
{ "index" : { "_index" : "books", "_id" : "104" } }
 {"BookID":104,"BookName":"无需流汗和吃苦3天成为java大神","BookPrice":15.5,"BookAuthor":"老张"}
}
```

使用sql的方式查询

```json
POST /_sql
{  
   "query": "SELECT * FROM books WHERE BookAuthor = '老王'"
}
```

![image-20220716153758925](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161537932.png)

```
POST /_sql/translate
{  
   "query": "SELECT * FROM books WHERE BookAuthor = '老王'"
}
```

此接口会将sql转换原本的es查询数据进行展示

![image-20220716154502057](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161545180.png)

### 22.7、es的外部访问

![image-20220716160742701](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161607738.png)

主要目的是加一层路径

```yaml
nginx.ingress.kubernetes.io/rewrite-target: /$1
```

测试

```go
package main

import (
	"context"
	"fmt"
	"github.com/olivere/elastic/v7"
	"log"
	"reflect"
)

type Books struct {
	BookID     int
	BookName   string
	BookPrice1 float64
	BookAuthor string
}

func MapToBooks(rsp *elastic.SearchResult) []*Books {
	ret := []*Books{}
	var t *Books
	for _, item := range rsp.Each(reflect.TypeOf(t)) {
		ret = append(ret, item.(*Books))
	}
	return ret
}
func getClient() *elastic.Client {
	client, err := elastic.NewSimpleClient(
		elastic.SetSniff(false),
		elastic.SetURL("http://es.xiongmao.com/es1/", "http://es.xiongmao.com/es2/"),
	)
	if err != nil {
		panic(err)
	}
	return client
}
func main() {

	matchQuery := elastic.NewMatchQuery("BookName", "java编程")
	rsp, err := getClient().Search().Index("books").Query(matchQuery).Do(context.Background())
	if err != nil {
		log.Fatal(err)
	}
	books := MapToBooks(rsp)
	for _, book := range books {
		fmt.Println(book)
	}

}

```

![image-20220716161036521](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161610969.png)

## 23、kubectl安装与控制rancher集群

kubecl下载地址 [https](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md)[://](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md)[github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md)

或者拷贝kubectl

```sh
docker  cp 565b61c7f5ad:/usr/bin/k3s  kubectl #拷贝rancher容器中的kubectl命令
mv kubectl /usr/bin/kubectl #拷贝命令
mkdir -p .kube && mv  mycluster.yaml .kube/config
vim  ~/.bash_profile  
```

```sh
KUBECONFIG=/root/.kube/config
PATH=$PATH:$HOME/bin
export KUBECONFIG
export PATH
```

```sh
 source ~/.bash_profile # 然后执行
```

```sh
kubectl cluster-info #测试
```

![image-20220716165833215](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/202207161658430.png)

