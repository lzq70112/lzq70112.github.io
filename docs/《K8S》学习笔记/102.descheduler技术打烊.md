---
title: descheduler技术打烊
date: 2021-09-18 10:13:16
permalink: /pages/K8S102/
categories:
  - 《K8S》学习笔记
tags:
  - k8s
  - 奇巧淫技
---



## 1、descheduler背景

Kubernetes 中的调度是挂起的 Pod 将绑定到节点的过程，由 Kubernetes 的一个组件执行，称为 kube-scheduler。调度程序的决定，无论 Pod 是否可以或不能调度，都由其可配置的策略指导，该策略由一系列规则组成，称为所谓的词和优先级。调度程序的决策受在出现新的 pod 进行调度时对其 Kubernetes 集群的视图的影响。由于 Kubernetes 实时动态，并且它们的状态会随同着时间而变化，出于各种原因，可能希望已经运行的pod移动到其他一些节点：

- 有一些节点使用不足或使用。
- 最初的调度不再适用，因为在节点上添加或删除了污点或标签，不再满足 pod/节点亲和性要求。
- 一些节点出现故障，它们的pod移动到其他节点。
- 新节点被添加到集群中。

因此，可能在集群中不太需要节点上安排了多个 pod。调度器根据其策略寻找可以移动的 Pod 调用其回调。请注意，在当前的实现中，调度器不会安排被调用的 pod 的替换，依赖于默认的调度程序。

## 2、descheduler驱逐策略

该策略包括适用于所有策略的通用配置

| name                        | 默认值  | 描述                                                         |
| --------------------------- | ------- | ------------------------------------------------------------ |
| `nodeSelector`              | `nil`   | 限制处理的节点                                               |
| `evictLocalStoragePods`     | `false` | 允许内有本地存储的豆荚                                       |
| `evictSystemCriticalPods`   | `false` | [警告：将盗用 Kubernetes 系统 Pod] 允许带有任何优先级的 Pod，包括像 kube-dns 这样的系统 Pod |
| `ignorePvcPods`             | `false` | PVC pod 设置是否应取消或忽略                                 |
| `maxNoOfPodsToEvictPerNode` | `nil`   | 从每个生命周期引发的最大 Pod 数（通过所有策略求和）          |

### 2.1、RemoveDuplicates

此策略确保关联ReplicaSet（RS）、ReplicationController（RC）、StatefulSet或job的pod，只有一个pod与在同一节点上运行，如果他们是重复的在同一节点则会被驱逐。

例如：如果节点down了，这些节点上的pod被移动到其他节点，导致多个pod与RS或RC关联（例如，在同一节点上运行），则可能会发生此问题。

一旦故障节点再次准备就绪，就可以启用此策略来逐出那些重复的pod。



它提供了一个可选参数`ExcludeOwnerTypes`，它是OwnerRef种类的列表。如果某个pod将这些类型中的任何一种列为OwnerRef，则该pod将不会被视为驱逐。

> 注意，Deployments创建的POD被认为是此策略的逐出对象。excludeOwnerKinds参数应包括ReplicaSet，排除Deployments创建的POD。

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "RemoveDuplicates":
     enabled: true
     params:
       removeDuplicates:
         excludeOwnerKinds:
         - "ReplicaSet"
```

当前，没有与该策略关联的参数，要禁用该策略，也很简单，只需要配置成 false 即可：

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "RemoveDuplicates":
     enabled: false
```



### 2.2、LowNodeUtilization

该策略查找未充分利用的节点，并从其他节点驱逐 Pod，以便 `kube-scheudler` 重新将它们调度到未充分利用的节点上。该策略的参数可以通过字段 `nodeResourceUtilizationThresholds` 进行配置。

节点的利用率不足可以通过配置 `thresholds` 阈值参数来确定，可以通过 CPU、内存和 Pod 数量的百分比进行配置。如果节点的使用率均低于所有阈值，则认为该节点未充分利用。

此外，还有一个可配置的阈值 `targetThresholds`，该阈值用于计算可从中驱逐 Pod 的那些潜在节点，对于所有节点 `thresholds` 和 `targetThresholds` 之间的阈值被认为是合理使用的，不考虑驱逐。`targetThresholds` 阈值也可以针对 CPU、内存和 Pod 数量进行配置。`thresholds` 和 `targetThresholds` 可以根据你的集群需求进行动态调整，如下所示示例：

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "LowNodeUtilization":
     enabled: true
     params:
       nodeResourceUtilizationThresholds:
         thresholds:
           "cpu" : 20
           "memory": 20
           "pods": 20
         targetThresholds:
           "cpu" : 50
           "memory": 50
           "pods": 50
```

和 `LowNodeUtilization` 策略关联的另一个参数是 `numberOfNodes`，只有当未充分利用的节点数大于该配置值的时候，才可以配置该参数来激活该策略，该参数对于大型集群非常有用，其中有一些节点可能会频繁使用或短期使用不足，默认情况下，`numberOfNodes` 为0。

### 2.3、HighNodeUtilization

该策略查找未充分利用的节点，并从节点中逐出POD，希望这些POD将被紧凑地调度到更少的节点中。此策略与节点自动缩放结合使用，旨在帮助触发未充分利用的节点的向下缩放。此策略必须与调度程序策略`MostRequestedPriority`一起使用。此策略的参数在`nodeResourceUtilizationThresholds`下配置。

节点利用率不足由可配置的阈值确定。可以根据百分比为cpu、内存、POD数量和扩展资源配置阈值。百分比计算为节点上请求的当前资源与可分配的总资源之比。对于pod，这意味着节点上的pod数量是该节点pod容量集的一部分。

如果一个节点的使用率低于所有节点的阈值（cpu、内存、POD数量和扩展资源），则该节点被视为未充分利用。目前，pods请求资源需求被考虑用于计算节点资源利用率。任何高于阈值的节点都被认为是适当利用的，不考虑逐出。

thresholds参数可以根据您的集群需求进行调整。

> 请注意，此策略将POD从未充分利用的节点（使用率低于阈值的节点）中逐出，以便可以在适当利用的节点中重新创建POD。

如果任何未充分利用的节点数或适当利用的节点数为零，该策略将中止。

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "HighNodeUtilization":
     enabled: true
     params:
       nodeResourceUtilizationThresholds:
         thresholds:
           "cpu" : 20
           "memory": 20
           "pods": 20
```

### 2.4、RemovePodsViolatingInterPodAntiAffinity

该策略可以确保从节点中删除违反 Pod 反亲和性的 Pod。比如某个节点上有 `podA` 这个 Pod，并且 `podB` 和 `podC`（在同一个节点上运行）具有禁止它们在同一个节点上运行的反亲和性规则，则 `podA` 将被从该节点上驱逐，以便 `podB` 和 `podC` 运行正常运行。当 `podB` 和 `podC` 已经运行在节点上后，反亲和性规则被创建就会发送这样的问题，目前没有和该策略相关联的配置参数，要禁用该策略，同样配置成 false 即可：

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "RemovePodsViolatingInterPodAntiAffinity":
     enabled: false
```

### 2.5、RemovePodsViolatingNodeAffinity

该策略确保从节点中删除违反节点亲和性的 Pod。比如名为 `podA` 的 Pod 被调度到了节点 `nodeA`，`podA` 在调度的时候满足了节点亲和性规则 `requiredDuringSchedulingIgnoredDuringExecution`，但是随着时间的推移，节点 `nodeA` 不再满足该规则了，那么如果另一个满足节点亲和性规则的节点 `nodeB` 可用，则 `podA` 将被从节点 `nodeA` 驱逐，如下所示的策略配置示例：

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "RemovePodsViolatingNodeAffinity":
    enabled: true
    params:
      nodeAffinityType:
      - "requiredDuringSchedulingIgnoredDuringExecution"
```

### 2.6、RemovePodsViolatingNodeTaints

该策略可以确保从节点中删除违反 `NoSchedule` 污点的 Pod。比如有一个名为 `podA` 的 Pod，通过配置容忍 `key=value:NoSchedule` 允许被调度到有该污点配置的节点上，如果节点的污点随后被更新或者删除了，则污点将不再被 Pods 的容忍满足，然后将被驱逐，如下所示配置策略：

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "RemovePodsViolatingNodeTaints":
    enabled: true
```

### 2.7、RemovePodsViolatingTopologySpreadConstraint

此策略确保从节点中逐出违反拓扑扩展约束的POD。具体地说，它尝试将平衡拓扑域所需的最小POD数逐出到每个约束的maxSkew内。此策略至少需要k8s版本1.18。

默认情况下，此策略仅处理硬约束，将参数includeSoftConstraints设置为true将包括软约束。

平衡拓扑域时不使用策略参数labelSelector，仅在逐出期间应用，以确定是否可以逐出pod。

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "RemovePodsViolatingTopologySpreadConstraint":
     enabled: true
     params:
       includeSoftConstraints: false
```

### 2.8、RemovePodsHavingTooManyRestarts

此策略确保从节点中删除重启次数过多的POD。例如，一个带有EBS/PD的pod无法将卷/磁盘连接到实例，那么该pod应该重新调度到其他节点。它的参数包括podRestartThreshold，它是pod应该被逐出的重启次数（在所有符合条件的容器上求和），以及includingInitContainers，它确定是否应该在该计算中考虑初始化容器重启。

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "RemovePodsHavingTooManyRestarts":
     enabled: true
     params:
       podsHavingTooManyRestarts:
         podRestartThreshold: 100
         includingInitContainers: true
```

### 2.9、PodLifeTime

此策略将驱逐比`maxPodLifeTimeSeconds`早的POD。

您还可以指定podStatusPhases以仅收回具有特定StatusPhases的pod，当前此参数仅限于Running和Pending。

```sh
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "PodLifeTime":
     enabled: true
     params:
       podLifeTime:
         maxPodLifeTimeSeconds: 86400
         podStatusPhases:
         - "Pending"
```



### 2.10、RemoveFailedPods

此策略将驱逐处于失败状态阶段的POD。您可以提供可选参数以按失败原因进行筛选。通过将可选参数`includingInitContainers`设置为true，可以将原因扩展为包括InitContainers的原因。您可以指定一个可选参数minPodLifeTimeSeconds，以收回早于指定秒数的pod。最后，您可以指定可选参数`ExcludeOwnerTypes`，如果某个pod将这些类型中的任何一种列为`OwnerRef`，则该pod将不会被考虑逐出。

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "RemoveFailedPods":
     enabled: true
     params:
       failedPods:
         reasons:
         - "NodeAffinity"
         includingInitContainers: true
         excludeOwnerKinds:
         - "Job"
         minPodLifeTimeSeconds: 3600
```

## 3、针对资源过滤

### 3.1、Namespace过滤

以下策略接受一个namespaces参数，该参数允许指定包含.

- `PodLifeTime`
- `RemovePodsHavingTooManyRestarts`
- `RemovePodsViolatingNodeTaints`
- `RemovePodsViolatingNodeAffinity`
- `RemovePodsViolatingInterPodAntiAffinity`
- `RemoveDuplicates`
- `RemovePodsViolatingTopologySpreadConstraint`
- `RemoveFailedPods`

示例

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "PodLifeTime":
     enabled: true
     params:
        podLifeTime:
          maxPodLifeTimeSeconds: 86400
        namespaces:
          include:
          - "namespace1"
          - "namespace2"
```

不包括

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "PodLifeTime":
     enabled: true
     params:
        podLifeTime:
          maxPodLifeTimeSeconds: 86400
        namespaces:
          exclude:
          - "namespace1"
          - "namespace2"
```

### 3.2、Priority过滤

所有策略都可以配置优先级阈值，只有低于阈值的 pod 才能被驱逐。您可以通过设置`thresholdPriorityClassName`（将阈值设置为给定优先级类的值）或`thresholdPriority`（直接设置阈值）参数来指定此阈值。默认情况下，此阈值设置为`system-cluster-critical`优先级的值。

注意：设置`evictSystemCriticalPods`为 true 会完全禁用优先级过滤。

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "PodLifeTime":
     enabled: true
     params:
        podLifeTime:
          maxPodLifeTimeSeconds: 86400
        thresholdPriority: 10000
```

### 3.3、Label 过滤

策略包括

- `PodLifeTime`
- `RemovePodsHavingTooManyRestarts`
- `RemovePodsViolatingNodeTaints`
- `RemovePodsViolatingNodeAffinity`
- `RemovePodsViolatingInterPodAntiAffinity`
- `RemovePodsViolatingTopologySpreadConstraint`
- `RemoveFailedPods`

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "PodLifeTime":
    enabled: true
    params:
      podLifeTime:
        maxPodLifeTimeSeconds: 86400
      labelSelector:
        matchLabels:
          component: redis
        matchExpressions:
          - {key: tier, operator: In, values: [cache]}
          - {key: environment, operator: NotIn, values: [dev]}
```

### 3.4、node Fit 过滤

以下策略接受`nodeFit`可以优化调度的布尔参数

- `RemoveDuplicates`
- `LowNodeUtilization`
- `HighNodeUtilization`
- `RemovePodsViolatingInterPodAntiAffinity`
- `RemovePodsViolatingNodeAffinity`
- `RemovePodsViolatingNodeTaints`
- `RemovePodsViolatingTopologySpreadConstraint`
- `RemovePodsHavingTooManyRestarts`
- `RemoveFailedPods`

如果设置为`true`descheduler 将考虑满足驱逐标准的 pod 是否适合其他节点，然后再驱逐它们。如果一个 Pod 不能被重新调度到另一个节点，它不会被驱逐。目前，在设置`nodeFit`为时考虑以下标准`true`：

- 一个`nodeSelector`的pod
- 任何`Tolerations`对pod和任何`Taints`其他节点上
- `nodeAffinity` 在pod上
- 是否有任何其他节点被标记为 `unschedulable`

```yaml
apiVersion: "descheduler/v1alpha1"
kind: "DeschedulerPolicy"
strategies:
  "LowNodeUtilization":
     enabled: true
     params:
       nodeResourceUtilizationThresholds:
         thresholds:
           "cpu" : 20
           "memory": 20
           "pods": 20
         targetThresholds:
           "cpu" : 50
           "memory": 50
           "pods": 50
```

## 4、pod驱逐的原则

- [关键 pod](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/)（将 priorityClassName 设置为 system-cluster-critical 或 system-node-critical）永远不会被驱逐（除非`evictSystemCriticalPods: true`已设置）。
- 不属于 ReplicationController、ReplicaSet(Deployment)、StatefulSet 或 Job 的 Pod（静态或镜像 Pod 或独立 Pod）永远不会被驱逐，因为这些 Pod 不会被重新创建。
- 与 DaemonSet 关联的 Pod 永远不会被驱逐。
- 具有本地存储的 Pod 永远不会被驱逐（除非`evictLocalStoragePods: true`已设置）。
- 带有 PVC 的 Pod 被驱逐（除非`ignorePvcPods: true`设置）。
- 在`LowNodeUtilization`和 中`RemovePodsViolatingInterPodAntiAffinity`，pod 会按优先级从低到高逐出，如果它们具有相同的优先级，则尽力而为的 pod 会在可爆裂和有保证的 pod 之前被逐出。
- 带有注释的所有类型的 pod`descheduler.alpha.kubernetes.io/evict`都有资格被驱逐。此注释用于覆盖防止驱逐的检查，用户可以选择驱逐哪个 pod。用户应该知道如何以及是否会重新创建 pod。
- 默认情况下不会驱逐具有非 nil DeletionTimestamp 的 Pod。

## 5、安装

### 5.1、安装metrics-server

```
https://github.com/kubernetes-sigs/metrics-server/archive/v0.3.6.tar.gz
tar -zxvf v0.3.6.tar.gz
```

修改Metrics-Server配置文件

```
cd metrics-server-0.3.6/deploy/1.8+/
vim metrics-server-deployment.yaml
```

vim metrics-server-deployment.yaml文件

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metrics-server
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    k8s-app: metrics-server
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  template:
    metadata:
      name: metrics-server
      labels:
        k8s-app: metrics-server
    spec:
      serviceAccountName: metrics-server
      volumes:
      # mount in tmp so we can safely use from-scratch images and/or read-only containers
      - name: tmp-dir
        emptyDir: {}
      containers:
      - name: metrics-server
        # 修改image 和 imagePullPolicy
        image: mirrorgooglecontainers/metrics-server-amd64:v0.3.6
        imagePullPolicy: IfNotPresent
        # 新增command配置
        command:
        - /metrics-server
        - --kubelet-insecure-tls
        - --kubelet-preferred-address-types=InternalDNS,InternalIP,ExternalDNS,ExternalIP,Hostname
        volumeMounts:
        - name: tmp-dir
          mountPath: /tmp
        # 新增resources配置
        resources:
          limits:
            cpu: 300m
            memory: 200Mi
          requests:
            cpu: 200m
            memory: 100Mi　　
```

安装Metrics-Server

```
kubectl apply -f metrics-server-0.3.6/deploy/1.8+/
```

测试

```sh
kubectl get --raw "/apis/metrics.k8s.io/v1beta1/pods" |jq
kubectl top nodes
```



### 5.2、安装descheduler

添加仓库

```shell
helm repo add descheduler https://kubernetes-sigs.github.io/descheduler/
helm install my-release --namespace kube-system descheduler/descheduler
```

介绍

此图表使用[Helm](https://helm.sh/)包管理器在[Kubernetes](http://kubernetes.io/)集群上引导[调度程序](https://github.com/kubernetes-sigs/descheduler/)cron 作业。

先决条件

- Kubernetes 1.14+

helm安装

要使用发行版名称安装helm`my-release`

```shell
helm install --namespace kube-system my-release descheduler/descheduler
```

该命令以默认配置在 Kubernetes 集群上部署*descheduler*。这配置 部分列出了可以在安装过程中配置的参数。

> **提示**：列出所有版本使用`helm list`

**helm卸载**

要卸载/删除`my-release`部署：

```shell
helm delete my-release
```

该命令删除与图表关联的所有 Kubernetes 组件并删除发布。

**配置参数**

下表列出了*调度器*图表的可配置参数及其默认值。

| 范围                           |                             描述                             | 默认                                 |
| ------------------------------ | :----------------------------------------------------------: | ------------------------------------ |
| `kind`                         |                     用作 CronJob 或部署                      | `CronJob`                            |
| `image.repository`             |                    要使用的 Docker 存储库                    | `k8s.gcr.io/descheduler/descheduler` |
| `image.tag`                    |                     要使用的 Docker 标签                     | `v[chart appVersion]`                |
| `image.pullPolicy`             |                     Docker 镜像拉取策略                      | `IfNotPresent`                       |
| `imagePullSecrets`             |                     Docker 存储库的秘密                      | `[]`                                 |
| `nameOverride`                 | 部分覆盖`descheduler.fullname`模板的字符串（将添加发布名称） | `""`                                 |
| `fullnameOverride`             |          完全覆盖`descheduler.fullname`模板的字符串          | `""`                                 |
| `cronJobApiVersion`            |                      CronJob API 组版本                      | `"batch/v1"`                         |
| `schedule`                     |            这个cron时间表运行*descheduler*上工作             | `"*/2 * * * *"`                      |
| `startingDeadlineSeconds`      |  如果设置，`startingDeadlineSeconds`则为*调度程序*作业配置   | `nil`                                |
| `successfulJobsHistoryLimit`   | 如果设置，`successfulJobsHistoryLimit`则为*调度程序*作业配置 | `nil`                                |
| `failedJobsHistoryLimit`       |   如果设置，`failedJobsHistoryLimit`则为*调度程序*作业配置   | `nil`                                |
| `deschedulingInterval`         | 如果使用 kind:Deployment，则设置连续调度程序执行之间的时间。 | `5m`                                 |
| `cmdOptions`                   |                传递给*descheduler*命令的选项                 | *参见 values.yaml*                   |
| `deschedulerPolicy.strategies` |                     要应用的*调度器*策略                     | *参见 values.yaml*                   |
| `priorityClassName`            |                要添加到 pod 的优先级类的名称                 | `system-cluster-critical`            |
| `rbac.create`                  |               如果`true`，创建和使用 RBAC 资源               | `true`                               |
| `podSecurityPolicy.create`     |              如果`true`，创建 PodSecurityPolicy              | `true`                               |
| `resources`                    |                调度器容器 CPU 和内存请求/限制                | *参见 values.yaml*                   |
| `serviceAccount.create`        |           如果`true`，为 cron 作业创建一个服务帐户           | `true`                               |
| `serviceAccount.name`          | 要使用的服务帐户的名称，如果未设置且 create 为 true，则使用 fullname 模板生成名称 | `nil`                                |
| `nodeSelector`                 |      用于在特定节点上运行调度程序 cronjob 的节点选择器       | `nil`                                |
| `tolerations`                  |          在特定节点上运行调度程序 cronjob 的容忍度           | `nil`                                |

默认策略生成

```yaml
  strategies:
    RemoveDuplicates:
      enabled: true
    RemovePodsViolatingNodeTaints:
      enabled: true
    RemovePodsViolatingNodeAffinity:
      enabled: true
      params:
        nodeAffinityType:
        - requiredDuringSchedulingIgnoredDuringExecution
    RemovePodsViolatingInterPodAntiAffinity:
      enabled: true
    LowNodeUtilization:
      enabled: true
      params:
        nodeResourceUtilizationThresholds:
          thresholds:
            cpu: 20
            memory: 20
            pods: 20
          targetThresholds:
            cpu: 50
            memory: 50
            pods: 50
```



## 6、测试驱逐

测试驱逐

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-basic
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```

```sh
kubectl apply -f nginx.yaml
kubectl get pod -n default -o wide
```

```
NAME                                      READY   STATUS    RESTARTS   AGE     IP               NODE                NOMINATED NODE   READINESS GATES
nginx-deployment-basic-76bf4969df-82kbf   1/1     Running   0          4m40s   10.244.206.168   k8s-10.165.24.170   <none>           <none>
nginx-deployment-basic-76bf4969df-m7nl6   1/1     Running   0          4m40s   10.244.37.104    k8s-10.165.24.169   <none>           <none>
nginx-deployment-basic-76bf4969df-tv6zv   1/1     Running   0          4m40s   10.244.205.218   k8s-10.165.24.168   <none>           <none>
```

![image-20211130152208364](https://raw.githubusercontent.com/lzq70112/images/master/blog/image-20211130152208364.png)

触发策略

![image-20211130200811536](https://raw.githubusercontent.com/lzq70112/images/master/blog/image-20211130200811536.png)

![image-20211130200733541](https://raw.githubusercontent.com/lzq70112/images/master/blog/image-20211130200733541.png)



```
I1130 12:04:10.866453       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1638273850\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1638273850\" (2021-11-30 11:04:10 +0000 UTC to 2022-11-30 11:04:10 +0000 UTC (now=2021-11-30 12:04:10.866373593 +0000 UTC))"
I1130 12:04:10.866600       1 secure_serving.go:195] Serving securely on [::]:10258
I1130 12:04:10.866742       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1130 12:04:10.892135       1 node.go:46] "Node lister returned empty list, now fetch directly"
I1130 12:04:10.900951       1 pod_antiaffinity.go:81] "Processing node" node="k8s-10.165.24.168"
I1130 12:04:10.968112       1 pod_antiaffinity.go:81] "Processing node" node="k8s-10.165.24.169"
I1130 12:04:11.040374       1 pod_antiaffinity.go:81] "Processing node" node="k8s-10.165.24.170"
I1130 12:04:11.111586       1 node_affinity.go:75] "Executing for nodeAffinityType" nodeAffinity="requiredDuringSchedulingIgnoredDuringExecution"
I1130 12:04:11.111629       1 node_affinity.go:80] "Processing node" node="k8s-10.165.24.168"
I1130 12:04:11.156870       1 node.go:169] "Pod fits on node" pod="default/nginx-deployment-basic-76bf4969df-gx7fk" node="k8s-10.165.24.168"
I1130 12:04:11.156917       1 node.go:169] "Pod fits on node" pod="default/nginx-deployment-basic-76bf4969df-kvhwq" node="k8s-10.165.24.168"
I1130 12:04:11.156985       1 node.go:169] "Pod fits on node" pod="kedacom-project-namespace/cephfs-provisioner-6cb45d4d4-jn2gl" node="k8s-10.165.24.168"
I1130 12:04:11.157042       1 node.go:169] "Pod fits on node" pod="kedacom-project-namespace/dol-elasticsearch-78-exporter-7977b8b59d-gc2jf" node="k8s-10.165.24.168"
I1130 12:04:11.157069       1 node.go:169] "Pod fits on node" pod="kedacom-project-namespace/dol-elasticsearch-exporter-58d4647d54-vpmsr" node="k8s-10.165.24.168"
I1130 12:04:11.157204       1 node.go:169] "Pod fits on node" pod="kube-system/kube-state-metrics-6986f4f675-zgwpg" node="k8s-10.165.24.168"
I1130 12:04:11.157249       1 node_affinity.go:80] "Processing node" node="k8s-10.165.24.169"
I1130 12:04:11.213063       1 node.go:169] "Pod fits on node" pod="kedacom-project-namespace/alertmanager-1" node="k8s-10.165.24.169"
I1130 12:04:11.213147       1 node.go:169] "Pod fits on node" pod="kedacom-project-namespace/dol-prometheus-0" node="k8s-10.165.24.169"
I1130 12:04:11.213275       1 node_affinity.go:80] "Processing node" node="k8s-10.165.24.170"
I1130 12:04:11.268428       1 node.go:169] "Pod fits on node" pod="default/nginx-deployment-basic-76bf4969df-l5qcn" node="k8s-10.165.24.170"
I1130 12:04:11.268494       1 node.go:169] "Pod fits on node" pod="kedacom-project-namespace/alertmanager-0" node="k8s-10.165.24.170"
I1130 12:04:11.268624       1 node.go:169] "Pod fits on node" pod="kedacom-project-namespace/dol-prometheus-1" node="k8s-10.165.24.170"
I1130 12:04:11.268848       1 node_taint.go:81] "Processing node" node="k8s-10.165.24.168"
I1130 12:04:11.312521       1 node_taint.go:81] "Processing node" node="k8s-10.165.24.169"
I1130 12:04:11.370280       1 node_taint.go:81] "Processing node" node="k8s-10.165.24.170"
I1130 12:04:11.608101       1 nodeutilization.go:164] "Node is underutilized" node="k8s-10.165.24.168" usage=map[cpu:4225m memory:34851Mi pods:33] usagePercentage=map[cpu:35.208333333333336 memory:54.18162323038457 pods:30]
I1130 12:04:11.608182       1 nodeutilization.go:167] "Node is overutilized" node="k8s-10.165.24.169" usage=map[cpu:5825m memory:52197Mi pods:42] usagePercentage=map[cpu:48.541666666666664 memory:81.14883899332538 pods:38.18181818181818]
I1130 12:04:11.608215       1 nodeutilization.go:167] "Node is overutilized" node="k8s-10.165.24.170" usage=map[cpu:5225m memory:48717Mi pods:51] usagePercentage=map[cpu:43.541666666666664 memory:75.73860546080873 pods:46.36363636363637]
I1130 12:04:11.608239       1 lownodeutilization.go:100] "Criteria for a node under utilization" CPU=60 Mem=60 Pods=60
I1130 12:04:11.608276       1 lownodeutilization.go:101] "Number of underutilized nodes" totalNumber=1
I1130 12:04:11.608309       1 lownodeutilization.go:114] "Criteria for a node above target utilization" CPU=62 Mem=62 Pods=62
I1130 12:04:11.608326       1 lownodeutilization.go:115] "Number of overutilized nodes" totalNumber=2
I1130 12:04:11.608375       1 nodeutilization.go:223] "Total capacity to be moved" CPU=3215 Mem=5273266749 Pods=35
I1130 12:04:11.608399       1 nodeutilization.go:226] "Evicting pods from node" node="k8s-10.165.24.169" usage=map[cpu:5825m memory:52197Mi pods:42]
I1130 12:04:11.608700       1 nodeutilization.go:229] "Pods on node" node="k8s-10.165.24.169" allPods=42 nonRemovablePods=40 removablePods=2
I1130 12:04:11.608745       1 nodeutilization.go:236] "Evicting pods based on priority, if they have same priority, they'll be evicted based on QoS tiers"
I1130 12:04:11.649119       1 evictions.go:130] "Evicted pod" pod="kedacom-project-namespace/alertmanager-1" reason="LowNodeUtilization"
I1130 12:04:11.649416       1 nodeutilization.go:269] "Evicted pods" pod="kedacom-project-namespace/alertmanager-1" err=<nil>
I1130 12:04:11.649457       1 nodeutilization.go:294] "Updated node usage" node="k8s-10.165.24.169" CPU=5725 Mem=54195650560 Pods=41
I1130 12:04:11.650289       1 event.go:291] "Event occurred" object="kedacom-project-namespace/alertmanager-1" kind="Pod" apiVersion="v1" type="Normal" reason="Descheduled" message="pod evicted by sigs.k8s.io/deschedulerLowNodeUtilization"
I1130 12:04:11.672580       1 evictions.go:130] "Evicted pod" pod="kedacom-project-namespace/dol-prometheus-0" reason="LowNodeUtilization"
I1130 12:04:11.672854       1 nodeutilization.go:269] "Evicted pods" pod="kedacom-project-namespace/dol-prometheus-0" err=<nil>
I1130 12:04:11.673048       1 nodeutilization.go:294] "Updated node usage" node="k8s-10.165.24.169" CPU=5625 Mem=52048166912 Pods=40
I1130 12:04:11.673132       1 nodeutilization.go:240] "Evicted pods from node" node="k8s-10.165.24.169" evictedPods=2 usage=map[cpu:5625m memory:49637Mi pods:40]
I1130 12:04:11.673136       1 event.go:291] "Event occurred" object="kedacom-project-namespace/dol-prometheus-0" kind="Pod" apiVersion="v1" type="Normal" reason="Descheduled" message="pod evicted by sigs.k8s.io/deschedulerLowNodeUtilization"
I1130 12:04:11.673178       1 nodeutilization.go:226] "Evicting pods from node" node="k8s-10.165.24.170" usage=map[cpu:5225m memory:48717Mi pods:51]
I1130 12:04:11.673775       1 nodeutilization.go:229] "Pods on node" node="k8s-10.165.24.170" allPods=51 nonRemovablePods=48 removablePods=3
I1130 12:04:11.673829       1 nodeutilization.go:236] "Evicting pods based on priority, if they have same priority, they'll be evicted based on QoS tiers"
I1130 12:04:11.700919       1 evictions.go:130] "Evicted pod" pod="default/nginx-deployment-basic-76bf4969df-l5qcn" reason="LowNodeUtilization"
I1130 12:04:11.701078       1 nodeutilization.go:269] "Evicted pods" pod="default/nginx-deployment-basic-76bf4969df-l5qcn" err=<nil>
I1130 12:04:11.701126       1 nodeutilization.go:294] "Updated node usage" node="k8s-10.165.24.170" CPU=5225 Mem=51083476992 Pods=50
I1130 12:04:11.701257       1 event.go:291] "Event occurred" object="default/nginx-deployment-basic-76bf4969df-l5qcn" kind="Pod" apiVersion="v1" type="Normal" reason="Descheduled" message="pod evicted by sigs.k8s.io/deschedulerLowNodeUtilization"
I1130 12:04:11.728656       1 evictions.go:130] "Evicted pod" pod="kedacom-project-namespace/alertmanager-0" reason="LowNodeUtilization"
I1130 12:04:11.728832       1 nodeutilization.go:269] "Evicted pods" pod="kedacom-project-namespace/alertmanager-0" err=<nil>
I1130 12:04:11.728873       1 nodeutilization.go:294] "Updated node usage" node="k8s-10.165.24.170" CPU=5125 Mem=50546606080 Pods=49
I1130 12:04:11.729218       1 event.go:291] "Event occurred" object="kedacom-project-namespace/alertmanager-0" kind="Pod" apiVersion="v1" type="Normal" reason="Descheduled" message="pod evicted by sigs.k8s.io/deschedulerLowNodeUtilization"
I1130 12:04:11.771287       1 evictions.go:130] "Evicted pod" pod="kedacom-project-namespace/dol-prometheus-1" reason="LowNodeUtilization"
I1130 12:04:11.771445       1 nodeutilization.go:269] "Evicted pods" pod="kedacom-project-namespace/dol-prometheus-1" err=<nil>
I1130 12:04:11.771478       1 nodeutilization.go:294] "Updated node usage" node="k8s-10.165.24.170" CPU=5025 Mem=48399122432 Pods=48
I1130 12:04:11.771525       1 nodeutilization.go:240] "Evicted pods from node" node="k8s-10.165.24.170" evictedPods=3 usage=map[cpu:5025m memory:46157Mi pods:48]
I1130 12:04:11.771574       1 lownodeutilization.go:163] "Total number of pods evicted" evictedPods=5
I1130 12:04:11.771599       1 descheduler.go:152] "Number of evicted pods" totalEvicted=5
```

