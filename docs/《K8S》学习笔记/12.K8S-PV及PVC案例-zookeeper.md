---

title: K8S-PV及PVC案例-zookeeper
date: 2021-09-08 10:13:16
permalink: /pages/b5e1556/
categories:
  - 《K8S》学习笔记
tags:
  - k8s
  - zookeeper
---

<!-- more -->

## 1、概念

默认情况下容器中的磁盘文件是非持久化的，对于运行在容器中的应用来说面临两个问题，第一：当容器挂掉kubelet将重启启动它时，文件将会丢失；第二：当Pod中同时运行多个容器，容器之间需要共享文件时，Kubernetes的Volume解决了这两个问题

[官方文档](https://kubernetes.io/zh/docs/concepts/storage/)

- PersistentVolume（PV）是集群中已由管理员配置的一段网络存储，集群中的存储资源就像一个node节点是一个集群资源，PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期， 该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统，PV是由管理员添加的的一个存储的描述，是一个全局资源即不隶属于任何namespace，包含存储的类型，存储的大小和访问模式等，它的生命周期独立于Pod，例如当使用它的Pod销毁时对PV没有影响。 
- PersistentVolumeClaim（PVC）是用户存储的请求，它类似于pod，Pod消耗节点资源，PVC消耗存储资源， 就像pod可以请求特定级别的资源（CPU和内存），PVC是namespace中的资源，可以设置特定的空间大小和访问模式。
- kubernetes 从1.0版本开始支持PersistentVolume和PersistentVolumeClaim。
-  PV是对底层网络存储的抽象，即将网络存储定义为一种存储资源，将一个整体的存储资源拆分成多份后给不同的业务使用。
-  PVC是对PV资源的申请调用，就像POD消费node节点资源一样,pod是通过PVC将数据保存至PV，PV在保存至存储。

![image-20210908143522158](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/image-20210908143522158.png)

## 2、PersistentVolume参数

```
accessModes ：PVC 访问模式，#kubectl explain PersistentVolumeClaim.spec.volumeMode
    ReadWriteOnce – PVC只能被单个节点以读写权限挂载，RWO
    ReadOnlyMany – PVC以可以被多个节点挂载但是权限是只读的,ROX
    ReadWriteMany – PVC可以被多个节点是读写方式挂载使用,RWX
#kubectl explain PersistentVolume.spec.persistentVolumeReclaimPolicy
Retain – 删除PV后保持原装，最后需要管理员手动删除
Recycle – 空间回收，及删除存储卷上的所有数据(包括目录和隐藏文件),目前仅支持NFS和hostPath
Delete – 自动删除存储卷
volumeMode #卷类型，kubectl explain PersistentVolume.spec.volumeMode
定义存储卷使用的文件系统是块设备还是文件系统，默认为文件系统
mountOptions #附加的挂载选项列表，实现更精细的权限控制
ro #等
```





```
#kubectl explain PersistentVolumeClaim.
accessModes ：PVC 访问模式，#kubectl explain PersistentVolumeClaim.spec.volumeMode
   ReadWriteOnce – PVC只能被单个节点以读写权限挂载，RWO
   ReadOnlyMany – PVC以可以被多个节点挂载但是权限是只读的,ROX
   ReadWriteMany – PVC可以被多个节点是读写方式挂载使用,RWX
   
resources： #定义PVC创建存储卷的空间大小

selector： #标签选择器，选择要绑定的PV
  matchLabels #匹配标签名称
  matchExpressions #基于正则表达式匹配
 
volumeName #要绑定的PV名称
volumeMode #卷类型
 定义PVC使用的文件系统是块设备还是文件系统，默认为文件系统


```

## 3、基于PV、PVC制作ZK集群

### 3.1、部署java基础镜像

》》》[harbor拉取自动脚本](https://hongwei888.com/pages/b5e154215/)《《《

`bash harbor-container.sh elevy/slim_java:8`



`vim Dockerfile`

```
FROM my.harbor.com/base/elevy/slim_java:8

ENV ZK_VERSION 3.5.9

RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && \
    apk add --no-cache --virtual .build-deps \
      ca-certificates   \
      gnupg             \
      tar               \
      wget &&           \
    #
    # Install dependencies
    apk add --no-cache  \
      bash &&           \
    #
    # Download Zookeeper
   wget -nv -O /tmp/zk.tgz "https://downloads.apache.org/zookeeper/zookeeper-${ZK_VERSION}/apache-zookeeper-${ZK_VERSION}.tar.gz" && \
    wget -nv -O /tmp/zk.tgz.asc "https://downloads.apache.org/zookeeper/zookeeper-${ZK_VERSION}/apache-zookeeper-${ZK_VERSION}.tar.gz.asc" && \

    wget -nv -O /tmp/KEYS https://dist.apache.org/repos/dist/release/zookeeper/KEYS && \
    #
    # Verify the signature
    export GNUPGHOME="$(mktemp -d)" && \
    gpg -q --batch --import /tmp/KEYS && \
    gpg -q --batch --no-auto-key-retrieve --verify /tmp/zk.tgz.asc /tmp/zk.tgz && \
    #
    # Set up directories
    #
    mkdir -p /zookeeper/data /zookeeper/wal /zookeeper/log && \
    #
    # Install
    tar -x -C /zookeeper --strip-components=1 --no-same-owner -f /tmp/zk.tgz && \
    #
    # Slim down
    cd /zookeeper && \
    rm -rf \
      *.txt \
      *.xml \
      bin/README.txt \
      bin/*.cmd \
      conf/* \
      contrib \
      dist-maven \
      docs \
      lib/*.txt \
      lib/cobertura \
      lib/jdiff \
      recipes \
      src \
      zookeeper-*.asc \
      zookeeper-*.md5 \
      zookeeper-*.sha1 && \
    #
    # Clean up
    apk del .build-deps && \
    rm -rf /tmp/* "$GNUPGHOME"

ADD zoo.conf /zookeeper/conf/

ENV PATH=/zookeeper/bin:${PATH} \
    ZOO_LOG4J_PROP="INFO, CONSOLE, ROLLINGFILE" \
    JMXPORT=9010

CMD [ "zkServer.sh", "start" ]

EXPOSE 2181 2888 3888 9010
```



》》[zookeeper官方镜像](https://downloads.apache.org/zookeeper/)《《



`vim build-command.sh `

```
#!/bin/bash
docker build -t my.harbor.com/base/zookeeper:v3.5.9 .
sleep 1
docker push my.harbor.com/base/zookeeper:v3.5.9
```

`bash build-command.sh `







### 3.2、创建PV

**nfs创建共享目录**

`mkdir -pv /data/zk/zk{1..3}`

`vim /etc/exports `

```
/data/webapp *(rw,no_root_squash)
/data/zk *(rw,no_root_squash)
```



**创建PV**

`vim zookeeper-persistentvolume.yaml`

```
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: zookeeper-datadir-pv-1
  namespace: ns-uat
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce 
  nfs:
    server: my.nfs.com
    path: /data/zk/zk1

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: zookeeper-datadir-pv-2
  namespace: ns-uat
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: my.nfs.com
    path: /data/zk/zk2

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: zookeeper-datadir-pv-3
  namespace: ns-uat
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: my.nfs.com
    path: /data/zk/zk3
```

 

`kubectl apply -f zookeeper-persistentvolume.yaml `



`kubectl get pv`

```
NAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
zookeeper-datadir-pv-1   2Gi        RWO            Retain           Available                                   95s
zookeeper-datadir-pv-2   2Gi        RWO            Retain           Available                                   95s
zookeeper-datadir-pv-3   2Gi        RWO            Retain           Available   
```

### 3.3、创建PVC

`vim  zookeeper-persistentvolumeclaim.yaml `

```
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: zookeeper-datadir-pvc-1
  namespace: ns-uat
spec:
  accessModes:
    - ReadWriteOnce
  volumeName: zookeeper-datadir-pv-1
  resources:
    requests:
      storage: 2Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: zookeeper-datadir-pvc-2
  namespace: ns-uat
spec:
  accessModes:
    - ReadWriteOnce
  volumeName: zookeeper-datadir-pv-2
  resources:
    requests:
      storage: 2Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: zookeeper-datadir-pvc-3
  namespace: ns-uat
spec:
  accessModes:
    - ReadWriteOnce
  volumeName: zookeeper-datadir-pv-3
  resources:
    requests:
      storage: 2Gi
```





` kubectl get pv` #查看PV绑定

```
NAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS   REASON   AGE
zookeeper-datadir-pv-1   2Gi        RWO            Retain           Bound    ns-uat/zookeeper-datadir-pvc-1                           10m
zookeeper-datadir-pv-2   2Gi        RWO            Retain           Bound    ns-uat/zookeeper-datadir-pvc-2                           10m
zookeeper-datadir-pv-3   2Gi        RWO            Retain           Bound    ns-uat/zookeeper-datadir-pvc-3                           10
```

` kubectl get pvc -n ns-uat` #查看PVC状态

```
NAME                      STATUS   VOLUME                   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
zookeeper-datadir-pvc-1   Bound    zookeeper-datadir-pv-1   2Gi        RWO                           45s
zookeeper-datadir-pvc-2   Bound    zookeeper-datadir-pv-2   2Gi        RWO                           45s
zookeeper-datadir-pvc-3   Bound    zookeeper-datadir-pv-3   2Gi        RWO                           45s
```

### 3.4 、部署zk集群

`vim zookeeper.yaml `

```
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  namespace: ns-uat
spec:
  ports:
    - name: client
      port: 2181
  selector:
    app: zookeeper
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper1
  namespace: ns-uat
spec:
  type: NodePort        
  ports:
    - name: client
      port: 2181
      nodePort: 32181
    - name: followers
      port: 2888
    - name: election
      port: 3888
  selector:
    app: zookeeper
    server-id: "1"
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper2
  namespace: ns-uat
spec:
  type: NodePort        
  ports:
    - name: client
      port: 2181
      nodePort: 32182
    - name: followers
      port: 2888
    - name: election
      port: 3888
  selector:
    app: zookeeper
    server-id: "2"
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper3
  namespace: ns-uat
spec:
  type: NodePort        
  ports:
    - name: client
      port: 2181
      nodePort: 32183
    - name: followers
      port: 2888
    - name: election
      port: 3888
  selector:
    app: zookeeper
    server-id: "3"
---
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  name: zookeeper1
  namespace: ns-uat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
        server-id: "1"
    spec:
      volumes:
        - name: data
          emptyDir: {}
        - name: wal
          emptyDir:
            medium: Memory
      containers:
        - name: server
          image: my.harbor.com/base/zookeeper:v3.5.9
          imagePullPolicy: Always
          env:
            - name: MYID
              value: "1"
            - name: SERVERS
              value: "zookeeper1,zookeeper2,zookeeper3"
            - name: JVMFLAGS
              value: "-Xmx2G"
          ports:
            - containerPort: 2181
            - containerPort: 2888
            - containerPort: 3888
          volumeMounts:
          - mountPath: "/zookeeper/data"
            name: zookeeper-datadir-pvc-1 
      volumes:
        - name: zookeeper-datadir-pvc-1 
          persistentVolumeClaim:
            claimName: zookeeper-datadir-pvc-1
---
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  name: zookeeper2
  namespace: ns-uat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
        server-id: "2"
    spec:
      volumes:
        - name: data
          emptyDir: {}
        - name: wal
          emptyDir:
            medium: Memory
      containers:
        - name: server
          image: my.harbor.com/base/zookeeper:v3.5.9
          imagePullPolicy: Always
          env:
            - name: MYID
              value: "2"
            - name: SERVERS
              value: "zookeeper1,zookeeper2,zookeeper3"
            - name: JVMFLAGS
              value: "-Xmx2G"
          ports:
            - containerPort: 2181
            - containerPort: 2888
            - containerPort: 3888
          volumeMounts:
          - mountPath: "/zookeeper/data"
            name: zookeeper-datadir-pvc-2 
      volumes:
        - name: zookeeper-datadir-pvc-2
          persistentVolumeClaim:
            claimName: zookeeper-datadir-pvc-2
---
kind: Deployment
#apiVersion: extensions/v1beta1
apiVersion: apps/v1
metadata:
  name: zookeeper3
  namespace: ns-uat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
        server-id: "3"
    spec:
      volumes:
        - name: data
          emptyDir: {}
        - name: wal
          emptyDir:
            medium: Memory
      containers:
        - name: server
          image: my.harbor.com/base/zookeeper:v3.5.9
          imagePullPolicy: Always
          env:
            - name: MYID
              value: "3"
            - name: SERVERS
              value: "zookeeper1,zookeeper2,zookeeper3"
            - name: JVMFLAGS
              value: "-Xmx2G"
          ports:
            - containerPort: 2181
            - containerPort: 2888
            - containerPort: 3888
          volumeMounts:
          - mountPath: "/zookeeper/data"
            name: zookeeper-datadir-pvc-3
      volumes:
        - name: zookeeper-datadir-pvc-3
          persistentVolumeClaim:
           claimName: zookeeper-datadir-pvc-3
```

`kubectl apply -f zookeeper.yaml `





### 3.5 测试

`kubectl exec -it zookeeper2-744bcd7c7f-lfm8s  -n ns-uat zookeeper/bin/zkServer.sh status`

```
ZooKeeper JMX enabled by default
ZooKeeper remote JMX Port set to 9010
ZooKeeper remote JMX authenticate set to false
ZooKeeper remote JMX ssl set to false
ZooKeeper remote JMX log4j set to true
Using config: /zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
```

