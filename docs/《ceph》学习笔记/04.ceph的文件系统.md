---
title: ceph的文件系统
date: 2021-08-29 10:13:16
permalink: /pages/ceph4/
categories:
  - 《ceph》学习笔记
tags:
  - ceph
---
[官方地址](https://docs.ceph.com/en/pacific/install/manual-deployment/)

<!-- more -->

## 1、文件存储的概念

ceph文件系统提供了任何大小的符合posix标准的分布式文件系统，它使用Ceph RADOS存储数据。要实现ceph文件系统，需要一个正在运行的ceph存储集群和至少一个ceph元数据服务器(MDS)来管理其元数据并使其与数据分离，这有助于降低复杂性和提高可靠性。

libcephfs库在支持其多个客户机实现方面发挥重要作用。它具有本机linux内核驱动程序支持，因此客户机可以使用本机文件系统安装，例如使用mount命令。她与samba紧密集成，支持CIFS和SMB。Ceph FS使用cephfuse模块扩展到用户空间(FUSE)中的文件系统。它还允许使用libcephfs库与RADOS集群进行直接的应用程序交互。
只有Ceph FS才需要Ceph MDS，其他存储方法的块和基于对象的存储不需要MDS。Ceph MDS作为一个守护进程运行，它允许客户机挂载任意大小的POSIX文件系统。MDS不直接向客户端提供任何数据，数据服务仅OSD完成。

![image-20211108001155305](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/image-20211108001155305.png)

## 2、安装部署MDS集群

```sh
ceph-deploy mds create  ceph-1
ceph-deploy mds create  ceph-2
ceph-deploy mds create  ceph-3
```

此时再查看状态

```sh
ceph -s #查看状态
```

```

  cluster:
    id:     ad3d4441-898a-475c-bfe7-ee2fa625e2a0
    health: HEALTH_WARN
            too many PGs per OSD (256 > max 250)
 
  services:
    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3 (age 2h)
    mgr: ceph-1(active, since 7d), standbys: ceph-2, ceph-3
    mds:  3 up:standby
    osd: 3 osds: 3 up (since 4h), 3 in (since 7d)
    rgw: 1 daemon active (ceph-1)
 
  task status:
 
  data:
    pools:   7 pools, 256 pgs
    objects: 4.20k objects, 1.2 GiB
    usage:   7.1 GiB used, 33 GiB / 40 GiB avail
    pgs:     256 active+clean

```

## 3、创建文件系统

### 3.1、创建资源池

CephFS 需要两个 Pools - `cephfs-data` 和 `cephfs-metadata`, 分别存储文件数据和文件元数据:

一般 metadata pool 可以从相对较少的 PGs 启动, 之后可以根据需要增加 PGs。因为 metadata pool 存储着 CephFS 文件的元数据, 为了保证安全, 最好有较多的副本数。 为了能有较低的延迟, 可以考虑将 metadata 存储在 SSDs 上。

```
ceph osd pool create cephfs_data 256
ceph osd pool create cephfs_metadata 64
```

### 3.2、关连文件系统

```sh
ceph fs  new ceph-demo cephfs_metadata cephfs_data
# 格式为
# fs new <fs_name> <metadata> <data>
```



```sh
ceph fs ls #查看文件系统
```

```
name: ceph-demo, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
```

此时再去看ceph状态

```
 ceph -s
```

```sh
  cluster:
    id:     ad3d4441-898a-475c-bfe7-ee2fa625e2a0
    health: HEALTH_WARN
            too many PGs per OSD (384 > max 250)
 
  services:
    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3 (age 2h)
    mgr: ceph-1(active, since 7d), standbys: ceph-2, ceph-3
    mds: ceph-demo:1 {0=ceph-2=up:active} 2 up:standby
    osd: 3 osds: 3 up (since 4h), 3 in (since 7d)
    rgw: 1 daemon active (ceph-1)
 
  task status:
 
  data:
    pools:   9 pools, 384 pgs
    objects: 4.22k objects, 1.2 GiB
    usage:   7.1 GiB used, 33 GiB / 40 GiB avail
    pgs:     384 active+clean
```

## 4、FS系统文件内核挂载

以 `kernel client` 形式挂载 CephFS，可以手动用 mount 命令挂载 CephFS 或者通过 `/etc/fstab` 自动挂载 CephFS。

### 4.1、手动挂载

```sh
cat ceph.client.admin.keyring #获得admin密钥
```

建立挂载点, 例如 /cephfs:

```shell
mkdir -p /cephfs
```

挂载 CephFS.

```shell
mount -t ceph 192.168.2.122:6789:/ /cephfs -o name=admin,secret=AQDLfH5hfUK/DxAADqZMXfhzRL0KkS/dmjHsHQ==
```

验证 CephFS 已经成功挂载:

```shell
stat -f /cephfs
```

```
 ceph df
```



### 4.2、自动挂载

创建挂载点:

```shell
 mkdir -p /cephfs
```

```sh
vim /etc/ceph/cephfs.key #放入admin的secret
```

```
AQDLfH5hfUK/DxAADqZMXfhzRL0KkS/dmjHsHQ==
```

编辑 /etc/fstab 文件:

```shell
 echo "ceph-1:6789,ceph-2:6789,ceph-3:6789:/ /cephfs ceph name=admin,secretfile=/etc/ceph/cephfs.key,_netdev,noatime 0 0" | sudo tee -a /etc/fstab
```

重启或者:

```shell
 mount -a
```

## 5、ceph-fuse挂载

安装

```sh
 yum -y install ceph-fuse ceph #安装挂载工具
```

```sh
mkdir /mnt/ceph-fuse/ #创建挂载点
```

```sh
ceph-fuse -n client.admin -m ceph-1:6789,ceph-2:6789,ceph-3:6789 /mnt/ceph-fuse/ 
# 指定用户 指定mon 指定路径
```

```
ceph-fuse[1586465]: starting ceph client
2021-11-07 12:15:12.067 7ff7b06eaf80 -1 init, newargv = 0x555f6d17fb90 newargc=9
ceph-fuse[1586465]: starting fuse
```

```
 df -Th #查看
```

```
文件系统                类型            容量  已用  可用 已用% 挂载点
devtmpfs                devtmpfs        1.9G     0  1.9G    0% /dev
tmpfs                   tmpfs           1.9G     0  1.9G    0% /dev/shm
tmpfs                   tmpfs           1.9G   12M  1.9G    1% /run
tmpfs                   tmpfs           1.9G     0  1.9G    0% /sys/fs/cgroup
/dev/mapper/centos-root xfs              33G  2.2G   31G    7% /
/dev/sda1               xfs             197M  160M   37M   82% /boot
tmpfs                   tmpfs           1.9G   52K  1.9G    1% /var/lib/ceph/osd/ceph-0
/dev/rbd0               ext4            3.9G  1.1G  2.7G   28% /mnt/rbd-demo
tmpfs                   tmpfs           378M     0  378M    0% /run/user/0
192.168.2.122:6789:/    ceph            9.5G     0  9.5G    0% /cephfs
ceph-fuse               fuse.ceph-fuse  9.5G     0  9.5G    0% /mnt/ceph-fuse
```
