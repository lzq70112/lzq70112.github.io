---
title: ceph的安装
date: 2021-08-29 10:13:16
permalink: /pages/ceph1/
categories:
  - 《ceph》学习笔记
tags:
  - ceph
---
[官方地址](https://docs.ceph.com/en/pacific/install/manual-deployment/)

<!-- more -->

## 1、概念及架构

**概念**

Ceph存储集群由几个不同的软件守护进程组成，每个守护进程负责Ceph的一个独特功能并将值添加到相应的组件中。每个守护进程是彼此独立的。

Ceph中的一切都以**对象**的形式存储，而RADOS就负责存储这些对象，而不考虑它们的数据类型。RADOS层确保数据一致性和可靠性。对于数据一致性，它执行数据复制、故障检测和恢复，还包括数据在集群节点间的迁移和再平衡。

一旦应用程序访问Ceph集群执行写操作，数据将会以对象的形式存储在Ceph对象存储设备OSD中，这是Ceph集群中存储实际用户数据并响应客户端读操作请求的唯一组件。通过，一个OSD守护进程与集群的一个物理磁盘绑定。

**Ceph Monitor(MON)组件**

通过一系列的map来跟踪整个集群的健康状态，map的组件有：OSD，MON，PG，和CRUSH。所有节点都想monitor节点报告状态，monitor本身不存放实际数据。

**librados库**

是一种用来简化访问RADOS的方法，它目前支持PHP，Rudy，java，python，C和C++语言。它提供了Ceph存储集群的一个本地接口RADOS，并且是其他服务的基础，以及为CephFS提供POSIX接口。librados API支持直接访问RADOS，使得开发者能够创建自己的接口来访问Ceph集群存储。

**Ceph块设备**

原来叫RADOS块设备(RBD)，它对外提供块存储，它可以被映射、格式化进而像其他磁盘一样挂载到服务器。

**Ceph对象网关**

RADOS网关(RGW)，它提供一个兼容Amazon S3和OpenStack对象存储API(Swift)的restful API接口。RGW还支持多租户和OpenStack的Keystone身份验证服务。

**Ceph元数据服务器(MDS)**

负责跟踪文件层次结构并存储只供CephFS使用的元数据。Ceph块设备和RADOS网关不需要元数据。

**Ceph文件系统(CephFS)**

提供一个任意大小且兼容POSIX的分布式文件系统。CephFS依赖Ceph MDS来跟踪文件层次结构，即元数据。

**Ceph RADOS**

RADOS是Ceph存储系统的核心，也称为Ceph存储集群。Ceph所有优秀特性都是由RADOS提供的，包括分布式对象存储、高可用性、高可靠性、没有单点故障、自我修复以及自我管理等。Ceph的数据访问方法(如RBD、CephFS、RADOSGW和librados)的所有操作都是在RADOS层之上构建的。

当Ceph集群接收到来自客户端的写请求时，CRUSH算法首先计算出存储位置，以此决定应该将数据写入什么地方。然后这些信息传递到RADOS层进行进一步处理。最后，这些对象存储在OSD中。

**Ceph OSD**

Ceph的OSD是Ceph集群中最重要的一个基础组件，它负责将实际的数据以对象形式存储在每一个集群节点的物理磁盘中。Ceph集群包含多个OSD，对于任何读写操作，客户端首先向monitor请求集群的map，然后，它们就可以无须monitor的干预直接与OSD进行I/O操作。

Ceph的核心特性都始于OSD，根据配置的副本数，Ceph通过跨集群节点复制每个对象多次来提供可靠性，同时提供高可用性和容错性。OSD上的每个对象都有一个主副本和几个辅副本，辅副本分散在其他OSD上。在磁盘发生故障的时候，Ceph的OSD守护进程会自动与其他OSD通信，从而开始执行恢复操作。

关于Ceph存储架构先介绍到这里，感谢大家阅读，后面将继续为大家介绍Ceph存储系统的其他特性和功能，敬请关注下一期。谢谢！

**PG\PGP**

- PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数
- PG的增加会引起PG内的数据进行分裂，分裂到相同的OSD上新生成的PG当中
- PGP的增加会引起部分PG的分布进行变化，但是不会引起PG内对象的变动



### 1.1网络架构

![img](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/697113-20150927112658115-732178602.jpg)

### 1.2、存储三种接口

- **Object**：有原生的 API，而且也兼容 Swift 和 S3 的 API。
- **Block**：支持精简配置、快照、克隆。
- **File**：Posix 接口，支持快照。

![img](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8yMDk5MjAxLTA3ODQ2MmJjYzM5MTA0MjYucG5n)





## 2、初始化环境

部署环境

![image-20211031094357331](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/image-20211031094357331.png)

![img](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/abfa2cc44e044bed919619dbbe1bbc59.png)

### 2.1 ntp同步

**ceph-1节点上**

```sh
yum install ntp ntpdate -y #需要三个节点安装
systemctl restart ntpd
systemctl enable ntpd
ntpq -pn
```

**ceph2、3**

`vim /etc/ntp.conf `

```
server 1 192.168.2.122 iburst
```

```sh
systemctl restart ntpd
systemctl enable ntpd
```

### 2.2、yum源

```sh
wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo #yum源
wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo #epel源
```

`vim ceph.repo`

```sh
# 手动修改源
[noarch]
name=noarch
baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch
enabled=1
gpgcheck=0
[x86_64]
name=x86_64
baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/
enable=1
gpgcheck=0
```

```
scp * 192.168.2.123:/etc/yum.repos.d/

scp * 192.168.2.124:/etc/yum.repos.d/
```

## 3、部署集群

```sh
yum install python-setuptools ceph-deploy  snappy leveldb gdisk python-argparse gperftools-libs -y #ceph-1安装部署包
 mkdir ceph-deploy
cd ceph-deploy/
ceph-deploy
# 初始化新的集群指定\公共网络\集群通信网络\指定监控节点
ceph-deploy new --public-network 192.168.2.0/24 --cluster-network 192.168.1.0/24 ceph-1
```

```sh
yum install ceph ceph-mon ceph-mgr ceph-radosgw ceph-mds -y #所有节点安装软件
```



### 3.1、mon初始化

```sh
ceph-deploy mon create-initial #mon初始化生成证书
```

```sh
$ls
ceph.bootstrap-mds.keyring  ceph.bootstrap-rgw.keyring  ceph-deploy-ceph.log
ceph.bootstrap-mgr.keyring  ceph.client.admin.keyring   ceph.mon.keyring
ceph.bootstrap-osd.keyring  ceph.conf
```



```sh
ceph-deploy admin ceph-1 ceph-2 ceph-3 #拷贝证书文件到各个节点
ceph-deploy mgr create ceph-1 #在ceph-1创建mgr
```

`ceph -s`# 查看集群状态

```sh
  cluster:
    id:     ad3d4441-898a-475c-bfe7-ee2fa625e2a0
    health: HEALTH_WARN
            OSD count 0 < osd_pool_default_size 3
            mon is allowing insecure global_id reclaim
 
  services:
    mon: 1 daemons, quorum ceph-1 (age 7m)
    mgr: ceph-1(active, since 40s)
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:    
```

### 3.2、osd节点添加

```
 ceph-deploy osd create ceph-1 --data /dev/sdb  #将sdb加入到osd
```

此时查看集群状态有个osd ，其中告警原因osd小于3

`ceph -s`

```sh
  cluster:
    id:     ad3d4441-898a-475c-bfe7-ee2fa625e2a0
    health: HEALTH_WARN
            OSD count 1 < osd_pool_default_size 3
            mon is allowing insecure global_id reclaim
 
  services:
    mon: 1 daemons, quorum ceph-1 (age 10m)
    mgr: ceph-1(active, since 10m)
    osd: 1 osds: 1 up (since 28s), 1 in (since 28s)
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   1.0 GiB used, 19 GiB / 20 GiB avail
    pgs: 
```

```sh
# 加入其他节点的sdb
ceph-deploy osd create ceph-2 --data /dev/sdb
ceph-deploy osd create ceph-3 --data /dev/sdb
```

```
  cluster:
    id:     ad3d4441-898a-475c-bfe7-ee2fa625e2a0
    health: HEALTH_WARN
            mon is allowing insecure global_id reclaim
 
  services:
    mon: 1 daemons, quorum ceph-1 (age 34m)
    mgr: ceph-1(active, since 34m)
    osd: 3 osds: 3 up (since 30s), 3 in (since 30s)
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 37 GiB / 40 GiB avail
    pgs: 
```



`ceph osd tree`# osd 树状图

```
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF 
-1       0.03908 root default                            
-3       0.01949     host ceph-1                         
 0   hdd 0.01949         osd.0       up  1.00000 1.00000 
-5       0.00980     host ceph-2                         
 1   hdd 0.00980         osd.1       up  1.00000 1.00000 
-7       0.00980     host ceph-3                         
 2   hdd 0.00980         osd.2       up  1.00000 1.00000 
```

## 4、高可用集群-mon

```sh
# 加入mon节点
ceph-deploy mon add ceph-3 --address 192.168.2.124
ceph-deploy mon add ceph-2 --address 192.168.2.123
# 查看集群状态
ceph -s
```

```
  cluster:
    id:     ad3d4441-898a-475c-bfe7-ee2fa625e2a0
    health: HEALTH_WARN
            mons are allowing insecure global_id reclaim
 
  services:
    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3 (age 35s)
    mgr: ceph-1(active, since 90m)
    osd: 3 osds: 3 up (since 56m), 3 in (since 56m)
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 37 GiB / 40 GiB avail
    pgs:    
```

`ceph quorum_status --format json-pretty` #json输出

```json
{
    "election_epoch": 14,
    "quorum": [
        0,
        1,
        2
    ],
    "quorum_names": [
        "ceph-1",
        "ceph-2",
        "ceph-3"
    ],
    "quorum_leader_name": "ceph-1",
    /* ceph-1为leader */
    "quorum_age": 462,
    "monmap": {
        "epoch": 3,
        "fsid": "ad3d4441-898a-475c-bfe7-ee2fa625e2a0",
        "modified": "2021-10-31 09:08:33.482692",
        "created": "2021-10-31 07:23:54.861979",
        "min_mon_release": 14,
        "min_mon_release_name": "nautilus",
        "features": {
            "persistent": [
                "kraken",
                "luminous",
                "mimic",
                "osdmap-prune",
                "nautilus"
            ],
            "optional": []
        },
        "mons": [
            {
                "rank": 0,
                "name": "ceph-1",
                "public_addrs": {
                    "addrvec": [
                        {
                            "type": "v2",
                            "addr": "192.168.2.122:3300",
                            "nonce": 0
                        },
                        {
                            "type": "v1",
                            "addr": "192.168.2.122:6789",
                            "nonce": 0
                        }
                    ]
                },
                "addr": "192.168.2.122:6789/0",
                "public_addr": "192.168.2.122:6789/0"
            },
            {
                "rank": 1,
                "name": "ceph-2",
                "public_addrs": {
                    "addrvec": [
                        {
                            "type": "v2",
                            "addr": "192.168.2.123:3300",
                            "nonce": 0
                        },
                        {
                            "type": "v1",
                            "addr": "192.168.2.123:6789",
                            "nonce": 0
                        }
                    ]
                },
                "addr": "192.168.2.123:6789/0",
                "public_addr": "192.168.2.123:6789/0"
            },
            {
                "rank": 2,
                "name": "ceph-3",
                "public_addrs": {
                    "addrvec": [
                        {
                            "type": "v2",
                            "addr": "192.168.2.124:3300",
                            "nonce": 0
                        },
                        {
                            "type": "v1",
                            "addr": "192.168.2.124:6789",
                            "nonce": 0
                        }
                    ]
                },
                "addr": "192.168.2.124:6789/0",
                "public_addr": "192.168.2.124:6789/0"
            }
        ]
    }
}

```

`ceph mon dump`#详细的信息

```
epoch 3
fsid ad3d4441-898a-475c-bfe7-ee2fa625e2a0
last_changed 2021-10-31 09:08:33.482692
created 2021-10-31 07:23:54.861979
min_mon_release 14 (nautilus)
0: [v2:192.168.2.122:3300/0,v1:192.168.2.122:6789/0] mon.ceph-1
1: [v2:192.168.2.123:3300/0,v1:192.168.2.123:6789/0] mon.ceph-2
2: [v2:192.168.2.124:3300/0,v1:192.168.2.124:6789/0] mon.ceph-3
dumped monmap epoch 3
```

## 5、高可用集群的-mgr

`ceph-deploy mgr create ceph-2 ceph-3` #将ceph2、ceph3加入集群

mgr是以主从的形式互备。只有当主挂了，从才会由standbys转到active

```sh
  cluster:
    id:     ad3d4441-898a-475c-bfe7-ee2fa625e2a0
    health: HEALTH_WARN
            mons are allowing insecure global_id reclaim

  services:
    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3 (age 14m)
    mgr: ceph-1(active, since 104m), standbys: ceph-2, ceph-3
    # 一主2从
    osd: 3 osds: 3 up (since 70m), 3 in (since 70m)

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 37 GiB / 40 GiB avail
    pgs:     
```

