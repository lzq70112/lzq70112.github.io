---
title: ceph的osd扩容
date: 2021-08-29 10:13:16
permalink: /pages/ceph5/
categories:
  - 《ceph》学习笔记
tags:
  - ceph
---
[官方地址](https://docs.ceph.com/en/pacific/install/manual-deployment/)

<!-- more -->

## 1、查看磁盘信息

`ceph-deploy disk list ceph-1` #查看磁盘信息

```
[ceph-1][DEBUG ] connected to host: ceph-1 
[ceph-1][DEBUG ] detect platform information from remote host
[ceph-1][DEBUG ] detect machine type
[ceph-1][DEBUG ] find the location of an executable
[ceph-1][INFO  ] Running command: fdisk -l
[ceph-1][INFO  ] Disk /dev/sda: 37.6 GB, 37580963840 bytes, 73400320 sectors
[ceph-1][INFO  ] Disk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectors
[ceph-1][INFO  ] Disk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors
# 多余的盘
[ceph-1][INFO  ] Disk /dev/mapper/centos-root: 35.2 GB, 35219570688 bytes, 68788224 sectors
[ceph-1][INFO  ] Disk /dev/mapper/centos-swap: 2147 MB, 2147483648 bytes, 4194304 sectors
[ceph-1][INFO  ] Disk /dev/mapper/ceph--81838bff--e7a2--4188--85b3--6db72c15bb04-osd--block--f0a158bf--a59f--4f78--abd9--c6fe695248ff: 21.5 GB, 21470642176 bytes, 41934848 sectors
```

## 2、加入可用磁盘

```sh
ceph-deploy disk zap ceph-1 /dev/sdc # 加入磁盘
```

```
[ceph-1][DEBUG ] connected to host: ceph-1 
[ceph-1][DEBUG ] detect platform information from remote host
[ceph-1][DEBUG ] detect machine type
[ceph-1][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[ceph-1][DEBUG ] zeroing last few blocks of device
[ceph-1][DEBUG ] find the location of an executable
[ceph-1][INFO  ] Running command: /usr/sbin/ceph-volume lvm zap /dev/sdc
[ceph-1][WARNIN] --> Zapping: /dev/sdc
[ceph-1][WARNIN] --> --destroy was not specified, but zapping a whole device will remove the partition table
[ceph-1][WARNIN] Running command: /usr/bin/dd if=/dev/zero of=/dev/sdc bs=1M count=10 conv=fsync
[ceph-1][WARNIN] --> Zapping successful for: <Raw Device: /dev/sdc>
```

## 3、添加osd扩容

```sh
ceph-deploy osd create  ceph-1 --data /dev/sdc #创建osd扩容
```

```
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy osd create ceph-1 --data /dev/sdc
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  bluestore                     : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7cec88f830>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  block_wal                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  journal                       : None
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  host                          : ceph-1
[ceph_deploy.cli][INFO  ]  filestore                     : None
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7cecad98c0>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.cli][INFO  ]  data                          : /dev/sdc
[ceph_deploy.cli][INFO  ]  block_db                      : None
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  debug                         : False
[ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device /dev/sdc
[ceph-1][DEBUG ] connected to host: ceph-1 
[ceph-1][DEBUG ] detect platform information from remote host
[ceph-1][DEBUG ] detect machine type
[ceph-1][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[ceph_deploy.osd][DEBUG ] Deploying osd to ceph-1
[ceph-1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph-1][DEBUG ] find the location of an executable
[ceph-1][INFO  ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sdc
[ceph-1][WARNIN] Running command: /usr/bin/ceph-authtool --gen-print-key
[ceph-1][WARNIN] Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 25541fc6-853f-4d3a-b5a0-81028700478e
[ceph-1][WARNIN] Running command: /usr/sbin/vgcreate --force --yes ceph-4642dde2-93ff-4ba5-b1f8-3d5cca9b1176 /dev/sdc
[ceph-1][WARNIN]  stdout: Physical volume "/dev/sdc" successfully created.
[ceph-1][WARNIN]  stdout: Volume group "ceph-4642dde2-93ff-4ba5-b1f8-3d5cca9b1176" successfully created
[ceph-1][WARNIN] Running command: /usr/sbin/lvcreate --yes -l 5119 -n osd-block-25541fc6-853f-4d3a-b5a0-81028700478e ceph-4642dde2-93ff-4ba5-b1f8-3d5cca9b1176
[ceph-1][WARNIN]  stdout: Logical volume "osd-block-25541fc6-853f-4d3a-b5a0-81028700478e" created.
[ceph-1][WARNIN] Running command: /usr/bin/ceph-authtool --gen-print-key
[ceph-1][WARNIN] Running command: /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-3
[ceph-1][WARNIN] Running command: /usr/bin/chown -h ceph:ceph /dev/ceph-4642dde2-93ff-4ba5-b1f8-3d5cca9b1176/osd-block-25541fc6-853f-4d3a-b5a0-81028700478e
[ceph-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /dev/dm-3
[ceph-1][WARNIN] Running command: /usr/bin/ln -s /dev/ceph-4642dde2-93ff-4ba5-b1f8-3d5cca9b1176/osd-block-25541fc6-853f-4d3a-b5a0-81028700478e /var/lib/ceph/osd/ceph-3/block
[ceph-1][WARNIN] Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-3/activate.monmap
[ceph-1][WARNIN]  stderr: 2021-11-16 21:00:53.124 7f4966518700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.bootstrap-osd.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
[ceph-1][WARNIN] 2021-11-16 21:00:53.124 7f4966518700 -1 AuthRegistry(0x7f49600662f8) no keyring found at /etc/ceph/ceph.client.bootstrap-osd.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,, disabling cephx
[ceph-1][WARNIN]  stderr: got monmap epoch 3
[ceph-1][WARNIN] Running command: /usr/bin/ceph-authtool /var/lib/ceph/osd/ceph-3/keyring --create-keyring --name osd.3 --add-key AQBUYpRhZpSZDBAA1c0wU2QdQuuhx5/wDoK5gA==
[ceph-1][WARNIN]  stdout: creating /var/lib/ceph/osd/ceph-3/keyring
[ceph-1][WARNIN] added entity osd.3 auth(key=AQBUYpRhZpSZDBAA1c0wU2QdQuuhx5/wDoK5gA==)
[ceph-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-3/keyring
[ceph-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-3/
[ceph-1][WARNIN] Running command: /usr/bin/ceph-osd --cluster ceph --osd-objectstore bluestore --mkfs -i 3 --monmap /var/lib/ceph/osd/ceph-3/activate.monmap --keyfile - --osd-data /var/lib/ceph/osd/ceph-3/ --osd-uuid 25541fc6-853f-4d3a-b5a0-81028700478e --setuser ceph --setgroup ceph
[ceph-1][WARNIN]  stderr: 2021-11-16 21:00:53.717 7f25ea435a80 -1 bluestore(/var/lib/ceph/osd/ceph-3/) _read_fsid unparsable uuid
[ceph-1][WARNIN] --> ceph-volume lvm prepare successful for: /dev/sdc
[ceph-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-3
[ceph-1][WARNIN] Running command: /usr/bin/ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/ceph-4642dde2-93ff-4ba5-b1f8-3d5cca9b1176/osd-block-25541fc6-853f-4d3a-b5a0-81028700478e --path /var/lib/ceph/osd/ceph-3 --no-mon-config
[ceph-1][WARNIN] Running command: /usr/bin/ln -snf /dev/ceph-4642dde2-93ff-4ba5-b1f8-3d5cca9b1176/osd-block-25541fc6-853f-4d3a-b5a0-81028700478e /var/lib/ceph/osd/ceph-3/block
[ceph-1][WARNIN] Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-3/block
[ceph-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /dev/dm-3
[ceph-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-3
[ceph-1][WARNIN] Running command: /usr/bin/systemctl enable ceph-volume@lvm-3-25541fc6-853f-4d3a-b5a0-81028700478e
[ceph-1][WARNIN]  stderr: Created symlink from /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-3-25541fc6-853f-4d3a-b5a0-81028700478e.service to /usr/lib/systemd/system/ceph-volume@.service.
[ceph-1][WARNIN] Running command: /usr/bin/systemctl enable --runtime ceph-osd@3
[ceph-1][WARNIN]  stderr: Created symlink from /run/systemd/system/ceph-osd.target.wants/ceph-osd@3.service to /usr/lib/systemd/system/ceph-osd@.service.
[ceph-1][WARNIN] Running command: /usr/bin/systemctl start ceph-osd@3
[ceph-1][WARNIN] --> ceph-volume lvm activate successful for osd ID: 3
[ceph-1][WARNIN] --> ceph-volume lvm create successful for: /dev/sdc
[ceph-1][INFO  ] checking OSD status...
[ceph-1][DEBUG ] find the location of an executable
[ceph-1][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host ceph-1 is now ready for osd use.
```



```sh
ceph -s #查看集群osd是否为4个
```

```
  cluster:
    id:     ad3d4441-898a-475c-bfe7-ee2fa625e2a0
    health: HEALTH_WARN
            Degraded data redundancy: 729/13029 objects degraded (5.595%), 46 pgs degraded, 5 pgs undersized
 
  services:
    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3 (age 23m)
    mgr: ceph-2(active, since 24m), standbys: ceph-3, ceph-1
    mds: ceph-demo:1 {0=ceph-2=up:active} 2 up:standby
    osd: 4 osds: 4 up (since 2m), 4 in (since 2m); 26 remapped pgs
    rgw: 1 daemon active (ceph-1)
 
  task status:
 
  data:
    pools:   9 pools, 384 pgs
    objects: 4.34k objects, 1.2 GiB
    usage:   8.4 GiB used, 52 GiB / 60 GiB avail
    pgs:     729/13029 objects degraded (5.595%)
             706/13029 objects misplaced (5.419%)
             314 active+clean
             41  active+recovery_wait+degraded
             15  active+remapped+backfill_wait
             5   active+recovery_wait+undersized+degraded+remapped
             5   active+recovery_wait+remapped
             3   active+recovery_wait
             1   active+recovering+remapped
 
  io:
    recovery: 49 KiB/s, 7 objects/s
 
  progress:
    Rebalancing after osd.3 marked in
      [===================...........]
```

当您将 Ceph OSD 守护进程添加到 Ceph 存储集群时，集群映射会使用新的 OSD 进行更新。回到[Calculating PG IDs](https://docs.ceph.com/en/pacific/architecture/?highlight=Rebalancing #calculating-pg-ids)，这会改变集群映射。因此，它改变了对象的放置，因为它改变了计算的输入。下图描述了重新平衡 过程（尽管相当粗糙，因为它对大型集群的影响要小得多），其中一些但不是全部 PG 从现有 OSD（OSD 1 和 OSD 2）迁移到新 OSD（OSD 3） ）。即使在重新平衡时，CRUSH 也是稳定的。许多归置组仍保留其原始配置，并且每个 OSD 都会增加一些容量，因此在重新平衡完成后，新 OSD 上不会出现负载峰值。

![image-20211117101114828](https://raw.githubusercontent.com/lzq70112/images/master/blog/image-20211117101114828.png)







```sh
 ceph osd tree #查看各个节点的osd情况
```

```
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF 
-1       0.05856 root default                            
-3       0.03897     host ceph-1                         
 0   hdd 0.01949         osd.0       up  1.00000 1.00000 
 3   hdd 0.01949         osd.3       up  1.00000 1.00000 
-5       0.00980     host ceph-2                         
 1   hdd 0.00980         osd.1       up  1.00000 1.00000 
-7       0.00980     host ceph-3                         
 2   hdd 0.00980         osd.2       up  1.00000 1.00000 
```



## 4、关闭Rebalance

```sh
ceph osd set norebalance #关闭rebalance
ceph osd set nobackfill #关闭nobackfill 
```

```sh
ceph osd unset norebalance #取消关闭rebalance
ceph osd unset nobackfill #取消关闭nobackfill 
```

## 5、osd更换坏盘

### 5.1、查看坏的osd

```sh
dmesg #查看磁盘的一些系统信息
```



```sh
ceph osd perf #查看ceph osd的延迟
```

```
osd commit_latency(ms) apply_latency(ms) 
  3                  0                 0 
  0                  0                 0 
  2                  0                 0 
  1                  0                 0 
```

```sh
ceph osd out osd.3 #提出坏掉盘
ceph crush map #查看crush表
```

```
no valid command found; 10 closest matches:
config get <who> {<key>}
config dump
config set <who> <name> <value> {--force}
config rm <who> <name>
mgr count-metadata <property>
mgr versions
osd crush reweight <name> <float[0.0-]>
osd crush reweight-all
osd crush unlink <name> {<ancestor>}
osd crush rm <name> {<ancestor>}
Error EINVAL: invalid command
```



### 5.2、删除osd信息

```sh
ceph osd crush dump #查看crush map还是有osd的信息
```

```
            "name": "osd.3",
```

```sh
 ceph osd crush rm osd.3 #删除对应的
```

```
removed item id 3 name 'osd.3' from crush map
```



### 5.3、删除对应osd tree

```sh
ceph osd tree #查看
```

```
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF 
-1       0.03908 root default                            
-3       0.01949     host ceph-1                         
 0   hdd 0.01949         osd.0       up  1.00000 1.00000 
-5       0.00980     host ceph-2                         
 1   hdd 0.00980         osd.1       up  1.00000 1.00000 
-7       0.00980     host ceph-3                         
 2   hdd 0.00980         osd.2       up  1.00000 1.00000 
 3             0 osd.3             down        0 1.00000 
```

```sh
ceph osd  rm osd.3 #删除对应的osd tree中的信息
```

### 5.4、删掉对应auth list

```sh
ceph auth list #查看对应认证列表
```

```
osd.3
        key: AQBUYpRhZpSZDBAA1c0wU2QdQuuhx5/wDoK5gA==
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
```

```sh
 ceph auth del osd.3 #删除对应的认证列表
```



## 6、检查数据一致性

```sh
ceph pg scrub 9.37   #针对pg轻量scrub
ceph pg deep-scrub 9.37   #针对pg深度scrub
```

