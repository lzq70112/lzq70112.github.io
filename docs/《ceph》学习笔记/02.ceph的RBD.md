---
title: ceph的RBD
date: 2021-08-29 10:13:16
permalink: /pages/ceph2/
categories:
  - 《ceph》学习笔记
tags:
  - ceph
---
[官方地址](https://docs.ceph.com/en/pacific/install/manual-deployment/)

<!-- more -->

## 1、ceph的资源池作用



 Ceph 架构中，Ceph 客户端是直接读或者写存放在 OSD上的 RADOS 对象存储中的对象（data object）的，因此，Ceph 需要走完 **(Pool, Object) → (Pool, PG) → OSD set → OSD/Disk** 完整的链路，才能让 ceph client 知道目标数据 object的具体位置在哪里：



![RBD 数据写入流程](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/94e0cd5a8c1c4123bc1a640c78a37dd5.png)

## 2、创建、查看资源池

```sh
ceph osd pool create ceph-demo 64 64 #创建osd池指定pg数和pgp数
```

```sh
ceph osd lspools #查看资源池
```

```
1 ceph-demo
# id 名称
```

## 3、创建、修改PG和PGP

- PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数
- PG的增加会引起PG内的数据进行分裂，分裂到相同的OSD上新生成的PG当中
- PGP的增加会引起部分PG的分布进行变化，但是不会引起PG内对象的变动

```sh
ceph osd pool set ceph-demo pg_num 3 #修改pg数
ceph osd pool set ceph-demo pgp_num 64 #修改pgp数
ceph osd pool get ceph-demo pg_num #查看pg数
ceph osd pool get ceph-demo size #查看副本数
ceph osd pool get ceph-demo crush_rule #查看调度算法
```

```sh
crush_rule: replicated_rule
# 默认为复制算法
```

## 3、RBD创建和使用

```sh
rbd create -p ceph-demo --image rbd-demo.img --size 2G
rbd create  ceph-demo/rbd-demo-1.img --size 2G
# 两种方式都是创建RBD的方式
rbd rm  ceph-demo/rbd-demo-1.img  #删除rbd
rbd -p ceph-demo ls #验证是否创建
```

```
rbd-demo-1.img
rbd-demo.img
```



```sh
rbd info  ceph-demo/rbd-demo-1.img #查看RBD信息
```

```sh
rbd image 'rbd-demo-1.img':
        size 2 GiB in 512 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 3873135bd549
        block_name_prefix: rbd_data.3873135bd549
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        # centos7只支持layering的特性
        op_features: 
        flags: 
        create_timestamp: Sun Oct 31 11:10:16 2021
        access_timestamp: Sun Oct 31 11:10:16 2021
        modify_timestamp: Sun Oct 31 11:10:16 2021
```

注意：**以上centos7只支持layering的特性，我们需要删除其他特性**

```sh
rbd feature disable ceph-demo/rbd-demo.img exclusive-lock
rbd feature disable ceph-demo/rbd-demo.img object-map
rbd feature disable ceph-demo/rbd-demo.img fast-diff
rbd feature disable ceph-demo/rbd-demo.img deep-flatten
```



```sh
rbd map  ceph-demo/rbd-demo.img #测试挂载
```

```
/dev/rbd0
```



```sh
rbd device list #查看镜像挂载块情况
```

```
id pool      namespace image        snap device    
0  ceph-demo           rbd-demo.img -    /dev/rbd0 
```



此时我们得到了一个块设备，可以通过格式化来使用块设备

```sh
mkfs.ext4 /dev/rbd0 #格式化
```

```sh
mke2fs 1.42.9 (28-Dec-2013)
Discarding device blocks: 完成                            
文件系统标签=
OS type: Linux
块大小=4096 (log=2)
分块大小=4096 (log=2)
Stride=1024 blocks, Stripe width=1024 blocks
131072 inodes, 524288 blocks
26214 blocks (5.00%) reserved for the super user
第一个数据块=0
Maximum filesystem blocks=536870912
16 block groups
32768 blocks per group, 32768 fragments per group
8192 inodes per group
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912

Allocating group tables: 完成                            
正在写入inode表: 完成                            
Creating journal (16384 blocks): 完成
Writing superblocks and filesystem accounting information: 完成 
```

```sh
mkdir /mnt/rbd-demo 
mount /dev/rbd0 /mnt/rbd-demo
 df -Th #查看挂载
```

```sh
文件系统                类型      容量  已用  可用 已用% 挂载点
devtmpfs                devtmpfs  1.9G     0  1.9G    0% /dev
tmpfs                   tmpfs     1.9G     0  1.9G    0% /dev/shm
tmpfs                   tmpfs     1.9G   12M  1.9G    1% /run
tmpfs                   tmpfs     1.9G     0  1.9G    0% /sys/fs/cgroup
/dev/mapper/centos-root xfs        33G  2.0G   31G    6% /
/dev/sda1               xfs       197M  160M   37M   82% /boot
tmpfs                   tmpfs     378M     0  378M    0% /run/user/0
tmpfs                   tmpfs     1.9G   52K  1.9G    1% /var/lib/ceph/osd/ceph-0
/dev/rbd0               ext4      2.0G  6.0M  1.8G    1% /mnt/rbd-demo
```

## 4、块存储的扩容

```sh
 rbd resize  ceph-demo/rbd-demo.img --size 4G#扩容
```

```sh
 rbd info  ceph-demo/rbd-demo.img #查看RBD扩容信息
```

```
rbd image 'rbd-demo.img':
        size 4 GiB in 1024 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 386a45ee6679
        block_name_prefix: rbd_data.386a45ee6679
        format: 2
        features: layering
        op_features: 
        flags: 
        create_timestamp: Sun Oct 31 11:09:06 2021
        access_timestamp: Sun Oct 31 11:09:06 2021
        modify_timestamp: Sun Oct 31 11:09:06 2021     
```

RBD的块已经被扩容到4G，fdisk系统查看块设备的扩容

```sh
fdisk -l #查看分区
```

```
磁盘 /dev/sda：37.6 GB, 37580963840 字节，73400320 个扇区
Units = 扇区 of 1 * 512 = 512 bytes
扇区大小(逻辑/物理)：512 字节 / 512 字节
I/O 大小(最小/最佳)：512 字节 / 512 字节
磁盘标签类型：dos
磁盘标识符：0x000d37ca

   设备 Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048      411647      204800   83  Linux
/dev/sda2          411648    73400319    36494336   8e  Linux LVM

磁盘 /dev/sdb：21.5 GB, 21474836480 字节，41943040 个扇区
Units = 扇区 of 1 * 512 = 512 bytes
扇区大小(逻辑/物理)：512 字节 / 512 字节
I/O 大小(最小/最佳)：512 字节 / 512 字节


磁盘 /dev/mapper/centos-root：35.2 GB, 35219570688 字节，68788224 个扇区
Units = 扇区 of 1 * 512 = 512 bytes
扇区大小(逻辑/物理)：512 字节 / 512 字节
I/O 大小(最小/最佳)：512 字节 / 512 字节


磁盘 /dev/mapper/centos-swap：2147 MB, 2147483648 字节，4194304 个扇区
Units = 扇区 of 1 * 512 = 512 bytes
扇区大小(逻辑/物理)：512 字节 / 512 字节
I/O 大小(最小/最佳)：512 字节 / 512 字节


磁盘 /dev/mapper/ceph--81838bff--e7a2--4188--85b3--6db72c15bb04-osd--block--f0a158bf--a59f--4f78--abd9--c6fe695248ff：21.5 GB, 21470642176 字节，41934848 个扇区
Units = 扇区 of 1 * 512 = 512 bytes
扇区大小(逻辑/物理)：512 字节 / 512 字节
I/O 大小(最小/最佳)：512 字节 / 512 字节


磁盘 /dev/rbd0：4294 MB, 4294967296 字节，8388608 个扇区
Units = 扇区 of 1 * 512 = 512 bytes
扇区大小(逻辑/物理)：512 字节 / 512 字节
I/O 大小(最小/最佳)：4194304 字节 / 4194304 字节
```

继续查看文件系统扩容


```sh
df -Th #文件系统并没有扩容
```

```sh
文件系统                类型      容量  已用  可用 已用% 挂载点
devtmpfs                devtmpfs  1.9G     0  1.9G    0% /dev
tmpfs                   tmpfs     1.9G     0  1.9G    0% /dev/shm
tmpfs                   tmpfs     1.9G   12M  1.9G    1% /run
tmpfs                   tmpfs     1.9G     0  1.9G    0% /sys/fs/cgroup
/dev/mapper/centos-root xfs        33G  2.0G   31G    6% /
/dev/sda1               xfs       197M  160M   37M   82% /boot
tmpfs                   tmpfs     378M     0  378M    0% /run/user/0
tmpfs                   tmpfs     1.9G   52K  1.9G    1% /var/lib/ceph/osd/ceph-0
/dev/rbd0               ext4      2.0G  6.0M  1.8G    1% /mnt/rbd-demo
# 此处应该为4G
```

我们发现上面文件系统并没有扩容

```sh
resize2fs /dev/rbd0 #我们需要手动文件系统进行扩容
```

```
resize2fs 1.42.9 (28-Dec-2013)
Filesystem at /dev/rbd0 is mounted on /mnt/rbd-demo; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/rbd0 is now 1048576 blocks long.
```

```sh
df -Th #扩容查看
```

```sh
文件系统                类型      容量  已用  可用 已用% 挂载点
devtmpfs                devtmpfs  1.9G     0  1.9G    0% /dev
tmpfs                   tmpfs     1.9G     0  1.9G    0% /dev/shm
tmpfs                   tmpfs     1.9G   12M  1.9G    1% /run
tmpfs                   tmpfs     1.9G     0  1.9G    0% /sys/fs/cgroup
/dev/mapper/centos-root xfs        33G  2.0G   31G    6% /
/dev/sda1               xfs       197M  160M   37M   82% /boot
tmpfs                   tmpfs     378M     0  378M    0% /run/user/0
tmpfs                   tmpfs     1.9G   52K  1.9G    1% /var/lib/ceph/osd/ceph-0
/dev/rbd0               ext4      3.9G  8.0M  3.7G    1% /mnt/rbd-demo
# 已经扩容了
```



## 5、查看存储数据

```sh
 rbd info  ceph-demo/rbd-demo.img #查看RBD扩容信息
```

```sh
rbd image 'rbd-demo.img':
        size 4 GiB in 1024 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 386a45ee6679
        block_name_prefix: rbd_data.386a45ee6679
        # 前缀文件名
        format: 2
        features: layering
        op_features: 
        flags: 
        create_timestamp: Sun Oct 31 11:09:06 2021
        access_timestamp: Sun Oct 31 11:09:06 2021
        modify_timestamp: Sun Oct 31 11:09:06 2021    
```

```sh
rados -p ceph-demo ls |grep rbd_data.386a45ee6679 #查看ceph-demo资源池的块数据对象
```

![image-20211107105159022](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/image-20211107105159022.png)

```sh
 rados -p ceph-demo stat rbd_data.386a45ee6679.000000000000010f #查看对象大小
```

```
ceph-demo/rbd_data.386a45ee6679.000000000000010f mtime 2021-10-31 11:30:38.000000, size 4194304
```







```sh
up ceph osd map ceph-demo rbd_data.386a45ee6679.000000000000010f #查看对象由pg(1.10) -> up 到osd 1,0,2
```

```
osdmap e22 pool 'ceph-demo' (1) object 'rbd_data.386a45ee6679.000000000000010f' -> pg 1.e709f1d0 (1.10) -> up ([1,0,2], p1) acting ([1,0,2], p1)
```



```sh
ceph osd tree #查看osd树状图
```

```
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF 
-1       0.03908 root default                            
-3       0.01949     host ceph-1                         
 0   hdd 0.01949         osd.0       up  1.00000 1.00000 
-5       0.00980     host ceph-2                         
 1   hdd 0.00980         osd.1       up  1.00000 1.00000 
-7       0.00980     host ceph-3                         
 2   hdd 0.00980         osd.2       up  1.00000 1.00000 
```



写入数据 测试

```sh
cd /mnt/rbd-demo/
dd if=/dev/zero of=test.img bs=1M count=1024
```

```sh
rados -p ceph-demo ls |grep rbd_data.386a45ee6679 |wc -l #查看对象写入后数量
```

```
291
```

每个4Mx291 = 1164M

```sh
df -Th #已经快/dev/rbd0 
```

```
文件系统                类型      容量  已用  可用 已用% 挂载点
devtmpfs                devtmpfs  1.9G     0  1.9G    0% /dev
tmpfs                   tmpfs     1.9G     0  1.9G    0% /dev/shm
tmpfs                   tmpfs     1.9G   12M  1.9G    1% /run
tmpfs                   tmpfs     1.9G     0  1.9G    0% /sys/fs/cgroup
/dev/mapper/centos-root xfs        33G  2.0G   31G    6% /
/dev/sda1               xfs       197M  160M   37M   82% /boot
tmpfs                   tmpfs     1.9G   52K  1.9G    1% /var/lib/ceph/osd/ceph-0
/dev/rbd0               ext4      3.9G  1.1G  2.7G   28% /mnt/rbd-demo
tmpfs                   tmpfs     378M     0  378M    0% /run/user/0
```

## 6、故障排查

```sh
 ceph health detail #查看健康详情
```

```sh
HEALTH_WARN application not enabled on 1 pool(s); mons are allowing insecure global_id reclaim
POOL_APP_NOT_ENABLED application not enabled on 1 pool(s)
    application not enabled on pool 'ceph-demo'
    # 未定义接口类型
    use 'ceph osd pool application enable <pool-name> <app-name>', where <app-name> is 'cephfs', 'rbd', 'rgw', or freeform for custom applications.
AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED mons are allowing insecure global_id reclaim
    mon.ceph-1 has auth_allow_insecure_global_id_reclaim set to true
    mon.ceph-2 has auth_allow_insecure_global_id_reclaim set to true
    mon.ceph-3 has auth_allow_insecure_global_id_reclaim set to true
    # mon认证告警
```



```sh
ceph osd pool application enable ceph-demo rbd #对资源池定义一个种接口方式
```

```sh
ceph osd pool application get ceph-demo  #查看资源池接口类型
```

```
{
    "rbd": {}
}
```

接下来我们继续看第二个报错

```sh
ceph config set mon auth_allow_insecure_global_id_reclaim false #将不安全的监控认证关闭
```

```sh
ceph -s #最后我们在查看health状态
```

```
  cluster:
    id:     ad3d4441-898a-475c-bfe7-ee2fa625e2a0
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3 (age 5h)
    mgr: ceph-1(active, since 6d), standbys: ceph-2, ceph-3
    osd: 3 osds: 3 up (since 5h), 3 in (since 6d)
 
  data:
    pools:   1 pools, 64 pgs
    objects: 296 objects, 1.1 GiB
    usage:   6.3 GiB used, 34 GiB / 40 GiB avail
    pgs:     64 active+clean
```



```sh
ceph crash ls #等我们有crash 告警时可以查看到
ceph crash info xxx #查看crash详情
ceph crash archive-all #打包所有crash包
```

