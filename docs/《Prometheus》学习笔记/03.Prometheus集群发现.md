---
title: Prometheus集群监控
date: 2021-09-08 10:13:16
permalink: /pages/1234/
categories:
  - 《Prometheus》学习笔记
tags:
  - k8s
  - Prometheus
---

## 1、集群监控的指标

对于集群的监控一般我们需要考虑以下几个方面：

- Kubernetes 节点的监控：比如节点的 cpu、load、disk、memory 等指标
- 内部系统组件的状态：比如 kube-scheduler、kube-controller-manager、kubedns/coredns 等组件的详细运行状态
- 编排级的 metrics：比如 Deployment 的状态、资源请求、调度和 API 延迟等数据指标

## 2、监控方案

- cAdvisor：[cAdvisor](https://github.com/google/cadvisor)是`Google`开源的容器资源监控和性能分析工具，它是专门为容器而生，本身也支持 Docker 容器，在 Kubernetes 中，我们不需要单独去安装，cAdvisor 作为 kubelet 内置的一部分程序可以直接使用。

- Kube-state-metrics：[kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)通过监听 API Server 生成有关资源对象的状态指标，比如 Deployment、Node、Pod，需要注意的是 kube-state-metrics 只是简单提供一个 metrics 数据，并不会存储这些指标数据，所以我们可以使用 Prometheus 来抓取这些数据然后存储。

- metrics-server：metrics-server 也是一个集群范围内的资源数据聚合工具，是 Heapster 的替代品，同样的，metrics-server 也只是显示数据，并不提供数据存储服务。

  

不过 kube-state-metrics 和 metrics-server 之间还是有很大不同的，二者的主要区别如下：

- kube-state-metrics 主要关注的是业务相关的一些元数据，比如 Deployment、Pod、副本状态等
- metrics-server 主要关注的是[资源度量 API](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md) 的实现，比如 CPU、文件描述符、内存、请求延时等指标。

## 3、node_exporter

以通过[node_exporter](https://github.com/prometheus/node_exporter)来获取，顾名思义，node_exporter 就是抓取用于采集服务器节点的各种运行指标，目前 node_exporter 支持几乎所有常见的监控点，比如 conntrack，cpu，diskstats，filesystem，loadavg，meminfo，netstat等，详细的监控点列表可以参考其[Github repo](https://github.com/prometheus/node_exporter)。

`prome-node-exporter.yaml`

`

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: kube-ops
  labels:
    name: node-exporter
spec:
  selector:
    matchLabels:
      name: node-exporter
  template:
    metadata:
      labels:
        name: node-exporter
    spec:
      hostPID: true
      # 主机的指标数据
      hostIPC: true
      hostNetwork: true
      containers:
      - name: node-exporter
        image: prom/node-exporter:v0.16.0
        ports:
        - containerPort: 9100
        resources:
          requests:
            cpu: 0.15
        securityContext:
          privileged: true
          # 特权模式
        args:
        - --path.procfs
        - /host/proc
        - --path.sysfs
        - /host/sys
        - --collector.filesystem.ignored-mount-points
        - '"^/(sys|proc|dev|host|etc)($|/)"'
        volumeMounts:
        - name: dev
          mountPath: /host/dev
        - name: proc
          mountPath: /host/proc
        - name: sys
          mountPath: /host/sys
        - name: rootfs
          mountPath: /rootfs
      tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      volumes:
        - name: proc
        # 将主机路径挂载进pod进行采集
          hostPath:
            path: /proc
        - name: dev
          hostPath:
            path: /dev
        - name: sys
          hostPath:
            path: /sys
        - name: rootfs
          hostPath:
            path: /
```

`hostPID: true`、`hostIPC: true`、`hostNetwork: true`3个策略，用来使用主机的 PID namespace、IPC namespace 以及主机网络，这些 namespace 就是用于容器隔离的关键技术，要注意这里的 namespace 和集群中的 namespace 是两个完全不相同的概念。

将主机的`/dev`、`/proc`、`/sys`这些目录挂载到容器中，这些因为我们采集的很多节点数据都是通过这些文件夹下面的文件来获取到的，比如我们在使用`top`命令可以查看当前`cpu`使用情况，数据就来源于文件`/proc/stat`，使用`free`命令可以查看当前内存使用情况，其数据来源是来自`/proc/meminfo`文件。



`curl 127.0.0.1:9100/metrics` #测试本机的metrics

## 4、服务发现

多个节点上面都运行了 node-exporter 程序，如果通过一个 Service 来将数据收集到一起用静态配置的方式配置到 Prometheus 去中，就只会显示一条数据，我们得自己在指标数据中去过滤每个节点的数据，那么有没有一种方式可以让 Prometheus 去自动发现我们节点的 node-exporter 程序，并且按节点进行分组呢？**服务发现**。

在 Kubernetes 下，Promethues 通过与 Kubernetes API 集成，目前主要支持5种服务发现模式，

分别是：

- Node
- Service
- Pod
- Endpoints
- Ingress



我们通过 kubectl 命令可以很方便的获取到当前集群中的所有节点信息：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-ops
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s
    scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
    - job_name: 'redis'
      static_configs:
      - targets: ['redis:9121']
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      # 使用node 服务发现
      relabel_configs:
      - source_labels: [__address__]
      # 匹配address字段就是服务监控的字段
        regex: '(.*):10250'
        # 默认接口为10250 但是node_exporte接口为9100
        replacement: '${1}:9100'
        # 将10250 替换到9100
        target_label: __address__
        # 重新赋值
        action: replace
        # 替换
      - action: labelmap
      # 指定标签
        regex: __meta_kubernetes_node_label_(.+)
        # 匹配__meta_kubernetes_node_label_之后的标签
```

对于 kubernetes_sd_configs 下面可用的标签如下： 可用元标签：

```
__meta_kubernetes_node_name：节点对象的名称

_meta_kubernetes_node_label：节点对象中的每个标签

_meta_kubernetes_node_annotation：来自节点对象的每个注释

_meta_kubernetes_node_address：每个节点地址类型的第一个地址（如果存在） 
```

关于 kubernets_sd_configs 更多信息可以查看官方文档：[kubernetes_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#)





`curl -X POST "http://10.68.41.110:9090/-/reload"` #热更新

查看监控

![image-20211002122307851](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/image-20211002122307851.png)





**由于 kubelet 也自带了一些监控指标数据**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-ops
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s
    scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
    - job_name: 'redis'
      static_configs:
      - targets: ['redis:9121']
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      # 使用node 服务发现
      relabel_configs:
      - source_labels: [__address__]
      # 匹配address字段就是服务监控的字段
        regex: '(.*):10250'
        # 默认接口为10250 但是node_exporte接口为9100
        replacement: '${1}:9100'
        # 将10250 替换到9100
        target_label: __address__
        # 重新赋值
        action: replace
        # 替换
      - action: labelmap
      # 指定标签
        regex: __meta_kubernetes_node_label_(.+)
        # 匹配__meta_kubernetes_node_label_之后的标签
    - job_name: 'kubernetes-kubelet'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
```

![image-20211002124522077](https://cdn.jsdelivr.net/gh/lzq70112/images/blog/image-20211002124522077.png)

